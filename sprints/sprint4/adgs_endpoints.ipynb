{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b445d78-b8ce-459c-a34e-11af313fe1e3",
   "metadata": {},
   "source": [
    "# ADGS endpoints demo\n",
    "\n",
    "In this demo we will call the rs-server ADGS HTTP endpoints:\n",
    "\n",
    "  * List available ADGS products\n",
    "  * Download some products into local storage and S3 bucket\n",
    "  * Monitor the download status from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f49d4-b62b-419a-8868-9072a40962f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "endpoint=\"http://rs-server-adgs:8000/adgs/aux\" # rs-server host = the container name\n",
    "datetime=\"2014-01-01T12:00:00Z/2023-12-30T12:00:00Z\"\n",
    "\n",
    "# Define a reusable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de3560-f30b-42b4-88ea-7ed93030fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a terminal, to list the available ADGS products, we would use the curl command:\n",
    "!set -x && curl -X GET \"{endpoint}/search?datetime={datetime}\" -H \"accept: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b022b7-a206-48ac-81be-12264381b232",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also call the HTTP endpoints from the Swagger UI: http://localhost:8001/docs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e3adc-0ace-4343-8b47-3660aee4cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But let's do it in python so it's easier to parse results\n",
    "import requests\n",
    "import pprint \n",
    "\n",
    "# Call the \"search\" endpoint\n",
    "print (f\"Call: '{endpoint}/search' with: datetime={datetime!r}\")\n",
    "payload = {\n",
    "    \"datetime\":datetime,    \n",
    "}\n",
    "data = requests.get(f\"{endpoint}/search\", payload)\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"features\"]\n",
    "assert len(products) == 3\n",
    "\n",
    "# Print the first n products\n",
    "print (\"Result:\")\n",
    "pprint.PrettyPrinter(indent=4).pprint(products[:3])\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also take one product only\n",
    "# Call the \"search\" endpoint\n",
    "print (f\"Call: '{endpoint}/search' with: datetime={datetime!r}&limit=1\")\n",
    "payload = {\n",
    "    \"datetime\": datetime,   \n",
    "    \"limit\": 1, \n",
    "}\n",
    "data = requests.get(f\"{endpoint}/search\", payload)\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"features\"]\n",
    "assert len(products) == 1\n",
    "\n",
    "# Print the result\n",
    "print (\"Result:\")\n",
    "pprint.PrettyPrinter(indent=4).pprint(products)\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39806fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort by datetime or id, either descending or ascending\n",
    "# Call the \"search\" endpoint with a sortby set in descending order for datetime\n",
    "print (f\"Call: '{endpoint}/search' with: datetime={datetime!r}&sortby=-adgs:datetime\")\n",
    "payload = {\n",
    "    \"datetime\": datetime,   \n",
    "    \"sortby\": \"-adgs:datetime\", \n",
    "}\n",
    "data = requests.get(f\"{endpoint}/search\", payload)\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"features\"]\n",
    "assert len(products) == 3\n",
    "\n",
    "# Print the result\n",
    "print (\"Result:\")\n",
    "pprint.PrettyPrinter(indent=4).pprint(products)\n",
    "print(\"...\")\n",
    "\n",
    "# Call the \"search\" endpoint with a sortby set in ascending order for datetime\n",
    "print (f\"Call: '{endpoint}/search' with: datetime={datetime!r}&sortby=+adgs:datetime\")\n",
    "payload = {\n",
    "    \"datetime\": datetime,   \n",
    "    \"sortby\": \"+adgs:datetime\", \n",
    "}\n",
    "data = requests.get(f\"{endpoint}/search\", payload)\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"features\"]\n",
    "assert len(products) == 3\n",
    "\n",
    "# Print the result\n",
    "print (\"Result:\")\n",
    "pprint.PrettyPrinter(indent=4).pprint(products)\n",
    "print(\"...\")\n",
    "\n",
    "# Keep only the names\n",
    "product_names = []\n",
    "for product in products:\n",
    "    product_names.append(product[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee015f-5e13-47bd-ad7d-81baff771a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"search\" endpoint has initialised the database with the products info.\n",
    "# Call the \"status\" endpoint to get the info from the products name.\n",
    "all_status = []\n",
    "print (f\"Call: '{endpoint}/status' with: name='...'\")\n",
    "for name in product_names:\n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    all_status.append (data.json())\n",
    "\n",
    "# Print the first n status\n",
    "print (\"Result:\")\n",
    "pprint.PrettyPrinter(indent=4).pprint(all_status[:2])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03d397-7737-4d9d-ad11-235c4beed428",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the database using pgAdmin.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb527198-3896-4447-b758-ce54f2ae70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket.\n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "!pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    "    region_name=os.environ[\"S3_REGION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064243b-e760-4e63-8cee-6696614caf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name and sub-directories\n",
    "bucket_name = \"demo-bucket\"\n",
    "bucket_dir = \"adgs/data\"\n",
    "\n",
    "# Full bucket name + subdirs\n",
    "bucket_url = f\"s3://{bucket_name}/{bucket_dir}\"\n",
    "\n",
    "# The local download directory is passed as an environment variable\n",
    "from pathlib import Path\n",
    "local_download_dir = Path (os.environ[\"RSPY_WORKING_DIR\"]) / bucket_dir\n",
    "\n",
    "# Clean existing files\n",
    "def clean_existing():\n",
    "\n",
    "    # If the s3 bucket already exist, remove the existing products from it\n",
    "    if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "        for name in product_names:\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=f\"{bucket_dir}/{name}\")\n",
    "    \n",
    "    # Else create the bucket\n",
    "    else:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    \n",
    "    # Create it if missing\n",
    "    local_download_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Remove all local files if they exist\n",
    "    for name in product_names:\n",
    "        file = local_download_dir / name\n",
    "        if file.is_file():\n",
    "            file.unlink()\n",
    "\n",
    "import time\n",
    "\n",
    "# Check that the files were downloaded locally\n",
    "def check_existing_local():\n",
    "    \n",
    "    # Wait 1 second before that or sometimes it bugs.\n",
    "    time.sleep(1)\n",
    "    for name in product_names:\n",
    "        file = Path (local_download_dir) / name    \n",
    "        if not file.is_file():\n",
    "            raise RuntimeError (f\"{file} is missing locally\")\n",
    "        print (f\"{file} exists\")\n",
    "\n",
    "# Check that the files were uploaded into the S3 bucket.\n",
    "# This time the local files are not kept.\n",
    "def check_existing_s3():\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        all_s3_files = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "    except KeyError:\n",
    "        all_s3_files = []\n",
    "    for name in product_names:\n",
    "        bucket_file = f\"{bucket_dir}/{name}\"\n",
    "        if not bucket_file in all_s3_files:\n",
    "            raise RuntimeError (f\"s3://{bucket_name}/{bucket_file} is missing from the S3 bucket\")\n",
    "        print (f\"s3://{bucket_name}/{bucket_file} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5462b2-f0c8-476c-ad56-1a814af85dbe",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the s3 bucket using the minio console: http://127.0.0.1:9001/browser with:\n",
    "\n",
    "  * Username: _minio_\n",
    "  * Password: _Strong#Pass#1234_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11229535-95d3-4cd8-9199-532e5ba9b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Callable\n",
    "\n",
    "print (f\"Call: '{endpoint}' with: name='...' local={local_download_dir!r} obs={bucket_url!r}\")\n",
    "\n",
    "# Call the ADGS endpoint to download one product in background \n",
    "# and upload it (optional) to the S3 bucket.\n",
    "async def download_one(name: str, save_to_s3: bool):\n",
    "\n",
    "    params = {\"name\": name, \"local\": local_download_dir}\n",
    "    if save_to_s3:\n",
    "        params[\"obs\"] = bucket_url\n",
    "\n",
    "    data = requests.get(endpoint, params)\n",
    "    assert data.status_code == 200\n",
    "\n",
    "# Download everything in parallel\n",
    "async def download_all(save_to_s3: bool, download_one: Callable=download_one):\n",
    "    async with asyncio.TaskGroup() as group:\n",
    "        for name in product_names:\n",
    "            group.create_task(download_one (name, save_to_s3))\n",
    "\n",
    "    #\n",
    "    # In the meantime, call the \"status\" endpoint to get and print the download status.\n",
    "    #\n",
    "\n",
    "    all_done = False\n",
    "    while not all_done: \n",
    "\n",
    "        # Count the number of products not started, in progres etc ...\n",
    "        all_status = {\"NOT_STARTED\": 0, \"IN_PROGRESS\": 0, \"FAILED\": 0, \"DONE\": 0}\n",
    "        for name in product_names:\n",
    "            \n",
    "            # Call the \"status\" endpoint\n",
    "            data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "            assert data.status_code == 200\n",
    "            all_status[(data.json())[\"status\"]] += 1\n",
    "\n",
    "        # Print result\n",
    "        print (\" / \".join ([f\"{status}:{count}\" for status, count in all_status.items()]))\n",
    "\n",
    "        if (all_status[\"DONE\"] + all_status[\"FAILED\"]) >= len(product_names):\n",
    "            all_done = True\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "clean_existing()\n",
    "\n",
    "print (\"Download everything to the local directory, not s3:\")\n",
    "await (download_all(save_to_s3=False))\n",
    "\n",
    "check_existing_local()\n",
    "\n",
    "print (\"\\nDownload everything again, but this time upload to S3:\")\n",
    "await (download_all(save_to_s3=True))\n",
    "\n",
    "check_existing_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fda3d-c44d-4dbf-b8dc-a8321b7cfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with prefect\n",
    "!pip install prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ae747-188f-44c1-858a-17f0861efdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "\n",
    "@task\n",
    "async def download_one_with_prefect(name: str, save_to_s3: bool):\n",
    "    return await download_one(name, save_to_s3)\n",
    "\n",
    "@flow(name=\"download adgs products\")\n",
    "async def download_all_with_prefect(save_to_s3: bool):\n",
    "    return await download_all(save_to_s3, download_one_with_prefect)\n",
    "\n",
    "clean_existing()\n",
    "\n",
    "print (\"[Prefect] Download everything to the local directory, not s3:\")\n",
    "await (download_all_with_prefect(save_to_s3=False))\n",
    "\n",
    "check_existing_local()\n",
    "\n",
    "print (\"\\n[Prefect] Download everything again, but this time upload to S3:\")\n",
    "await (download_all_with_prefect(save_to_s3=True))\n",
    "\n",
    "check_existing_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135f5fe-3d7a-449a-9f6d-d6132dfc2148",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "Open the Prefect dashboard: http://127.0.0.1:4200\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d4303-1b6e-4472-9489-a028d318cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt_format = \"%Y-%m-%dT%H:%M:%S.%f\" # %z\n",
    "\n",
    "# Check timeliness by substracting download stop date - publishing date.\n",
    "# Call the \"status\" endpoint.\n",
    "print (\"Timeliness for:\")\n",
    "for name in product_names:    \n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    values = data.json()\n",
    "    publication = datetime.strptime (values[\"available_at_station\"], dt_format)\n",
    "    stop = datetime.strptime (values[\"download_stop\"], dt_format)\n",
    "    timeliness = stop - publication\n",
    "    print (f\"  - {name}: {timeliness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddfd8e-e32c-4f01-97ff-35d80c4d6900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
