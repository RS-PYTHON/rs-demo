{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b445d78-b8ce-459c-a34e-11af313fe1e3",
   "metadata": {},
   "source": [
    "# CADU endpoints demo\n",
    "\n",
    "In this demo we will call the rs-server CADU HTTP endpoints:\n",
    "\n",
    "  * List available CADU products\n",
    "  * Download some products into local storage and S3 bucket\n",
    "  * Monitor the download status from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f49d4-b62b-419a-8868-9072a40962f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "endpoint=\"http://rs-server:8000/cadip/CADIP/cadu\" # rs-server host = the container name\n",
    "start=\"2014-01-01T12:00:00.000Z\"\n",
    "stop=\"2023-12-30T12:00:00.000Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de3560-f30b-42b4-88ea-7ed93030fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a terminal, to list the available CADU products, we would use the curl command:\n",
    "!set -x && curl -X GET \"{endpoint}/list?start_date={start}&stop_date={stop}\" -H \"accept: application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e3adc-0ace-4343-8b47-3660aee4cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But let's do it in python so it's easier to parse results\n",
    "import requests\n",
    "import pprint \n",
    "\n",
    "# Call the \"list\" endpoint\n",
    "data = requests.get(f\"{endpoint}/list\", {\"start_date\": start, \"stop_date\": stop})\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"CADIP\"]\n",
    "assert len(products) == 10\n",
    "\n",
    "# Print the first n products\n",
    "pprint.PrettyPrinter(indent=4).pprint(products[:3])\n",
    "print(\"...\")\n",
    "\n",
    "# Keep only the names\n",
    "product_names = [name for id, name in products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee015f-5e13-47bd-ad7d-81baff771a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"list\" endpoint has initialised the database with the products info.\n",
    "# Call the \"status\" endpoint to get the info from the products name.\n",
    "all_status = []\n",
    "for name in product_names:\n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    all_status.append (data.json())\n",
    "\n",
    "# Print the first n status\n",
    "pprint.PrettyPrinter(indent=4).pprint(all_status[:2])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03d397-7737-4d9d-ad11-235c4beed428",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the database using pgAdmin.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb527198-3896-4447-b758-ce54f2ae70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket.\n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "!pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064243b-e760-4e63-8cee-6696614caf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name\n",
    "bucket_name = \"test-data\"\n",
    "\n",
    "# If the s3 bucket already exist, remove the existing products from it\n",
    "if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "    for name in product_names:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=name)\n",
    "\n",
    "# Else create the bucket\n",
    "else:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# The local download directory is passed as an environment variable\n",
    "local_download_dir = os.environ[\"RSPY_LOCAL_DOWNLOAD\"]\n",
    "\n",
    "# Remove all local files if they exist\n",
    "from pathlib import Path\n",
    "for name in product_names:\n",
    "    file = Path (local_download_dir) / name\n",
    "    if file.is_file():\n",
    "        file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549f2ca-cb6f-457f-a1cf-8b66e9426cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Call the CADIP endpoint to download one product in background \n",
    "# and upload it (optional) to the S3 bucket.\n",
    "async def download_one(name: str, save_to_s3: bool):\n",
    "\n",
    "    params = {\"name\": name, \"local\": local_download_dir}\n",
    "    # obs = the bucket URL, if requested\n",
    "    if save_to_s3:\n",
    "        params[\"obs\"] = f\"s3://{bucket_name}\"\n",
    "\n",
    "    data = requests.get(endpoint, params)\n",
    "    assert data.status_code == 200\n",
    "\n",
    "# In parallel, call the \"status\" endpoint to get and print the download status.\n",
    "async def print_status():\n",
    "\n",
    "    # Wait a second if the staus need to be passed \n",
    "    # from DONE to NOT_STARTED if we download several times.\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    all_done = False\n",
    "    while not all_done: \n",
    "\n",
    "        # Count the number of products not started, in progres etc ...\n",
    "        all_status = {\"NOT_STARTED\": 0, \"IN_PROGRESS\": 0, \"FAILED\": 0, \"DONE\": 0}\n",
    "        for name in product_names:\n",
    "            \n",
    "            # Call the \"status\" endpoint\n",
    "            data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "            assert data.status_code == 200\n",
    "            all_status[(data.json())[\"status\"]] += 1\n",
    "\n",
    "        # Print result\n",
    "        print (\" / \".join ([f\"{status}:{count}\" for status, count in all_status.items()]))\n",
    "\n",
    "        if all_status[\"DONE\"] == len(product_names):\n",
    "            all_done = True\n",
    "        else:\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "# Call everything in parallel\n",
    "async def download_all(save_to_s3: bool):\n",
    "    async with asyncio.TaskGroup() as group:\n",
    "        group.create_task (print_status())\n",
    "        for name in product_names:\n",
    "            group.create_task(download_one (name, save_to_s3))\n",
    "\n",
    "print (\"Download everything to the local directory, not s3:\")\n",
    "await (download_all(save_to_s3=False))\n",
    "\n",
    "# Check that the local files exist. \n",
    "# Wait 1 second before that or sometimes it bugs.\n",
    "await asyncio.sleep(1)\n",
    "for name in product_names:\n",
    "    file = Path (local_download_dir) / name    \n",
    "    if not file.is_file():\n",
    "        raise RuntimeException (f\"{file} is missing locally\")\n",
    "    print (f\"{file} exists\")\n",
    "\n",
    "print (\"\\nDownload everything again, but this time upload to S3:\")\n",
    "await (download_all(save_to_s3=True))\n",
    "\n",
    "# This time the local files are not kept locally, \n",
    "# but they should be uploaded into the S3 bucket.\n",
    "await asyncio.sleep(1)\n",
    "all_s3_filenames = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "for name in product_names:    \n",
    "    if not name in all_s3_filenames:\n",
    "        raise RuntimeException (f\"{file} is missing from the S3 bucket\")\n",
    "    print (f\"s3://{bucket_name}/{name} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5462b2-f0c8-476c-ad56-1a814af85dbe",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the s3 bucket using the minio console: http://127.0.0.1:9001/browser with:\n",
    "\n",
    "  * Username: _minio_\n",
    "  * Password: _Strong#Pass#1234_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d4303-1b6e-4472-9489-a028d318cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt_format = \"%Y-%m-%dT%H:%M:%S.%f\" # %z\n",
    "\n",
    "# Check timeliness by substracting download stop date - publishing date.\n",
    "# Call the \"status\" endpoint.\n",
    "print (\"Timeliness for:\")\n",
    "for name in product_names:    \n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    values = data.json()\n",
    "    publication = datetime.strptime (values[\"available_at_station\"], dt_format)\n",
    "    stop = datetime.strptime (values[\"download_stop\"], dt_format)\n",
    "    timeliness = stop - publication\n",
    "    print (f\"  - {name}: {timeliness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ec4dc-8a2f-41bc-b113-0ba34f8a68ce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76dae95-cb96-4905-bf6d-ce3a18cc2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from prefect import flow\n",
    "\n",
    "\n",
    "@flow(retries=3, retry_delay_seconds=5, log_prints=True)\n",
    "def get_repo_info(repo_name: str = \"PrefectHQ/prefect\"):\n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    response = httpx.get(url)\n",
    "    response.raise_for_status()\n",
    "    repo = response.json()\n",
    "    print(f\"{repo_name} repository statistics ü§ì:\")\n",
    "    print(f\"Stars üå† : {repo['stargazers_count']}\")\n",
    "    print(f\"Forks üç¥ : {repo['forks_count']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_repo_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c002f-6453-43ae-82db-a4e08b911c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "import requests\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# Define some variables\n",
    "endpoint=\"http://rs-server:8000/cadip/CADIP/cadu\" # rs-server host = the container name\n",
    "start=\"2014-01-01T12:00:00.000Z\"\n",
    "stop=\"2023-12-30T12:00:00.000Z\"\n",
    "bucket_name = \"test-data\"\n",
    "local_download_dir = os.environ[\"RSPY_LOCAL_DOWNLOAD\"]\n",
    "\n",
    "@task(name='search_cadu_task')\n",
    "def search_cadu(date_start: str, date_end: str):\n",
    "    print(f\"Searching products between {date_start} and {date_end}\")\n",
    "    data = requests.get(f\"{endpoint}/list\", {\"start_date\": start, \"stop_date\": stop})\n",
    "    products = data.json()[\"CADIP\"]\n",
    "    print(f\"Here is the list of products found: {products}\")\n",
    "    product_names = [name for id, name in products]\n",
    "    return product_names\n",
    "\n",
    "@task(name='download_one_task')\n",
    "async def download_one(name: str, save_to_s3: bool):\n",
    "    params = {\"name\": name, \"local\": local_download_dir}\n",
    "    # obs = the bucket URL, if requested\n",
    "    if save_to_s3:\n",
    "        print(f\"pushing {name} to the bucket {bucket_name} ...\")\n",
    "        params[\"obs\"] = f\"s3://{bucket_name}/Cadip_products\"\n",
    "    data = requests.get(endpoint, params)\n",
    "    assert data.status_code == 200\n",
    "\n",
    "@task(name='print_status_task')\n",
    "async def print_status(product_names: list):\n",
    "    # Wait a second if the staus need to be passed \n",
    "    # from DONE to NOT_STARTED if we download several times.\n",
    "    await asyncio.sleep(1)\n",
    "    all_done = False\n",
    "    while not all_done: \n",
    "        # Count the number of products not started, in progres etc ...\n",
    "        all_status = {\"NOT_STARTED\": 0, \"IN_PROGRESS\": 0, \"FAILED\": 0, \"DONE\": 0}\n",
    "        for name in product_names:           \n",
    "            # Call the \"status\" endpoint\n",
    "            data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "            assert data.status_code == 200\n",
    "            all_status[(data.json())[\"status\"]] += 1\n",
    "        # Print result\n",
    "        print (\" / \".join ([f\"{status}:{count}\" for status, count in all_status.items()]))\n",
    "        if all_status[\"DONE\"] == len(product_names):\n",
    "            all_done = True\n",
    "        else:\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "    \n",
    "@task(name='download_all_task')\n",
    "async def download_all(save_to_s3: bool, product_names: list):\n",
    "    async with asyncio.TaskGroup() as group:\n",
    "        group.create_task (print_status.fn(product_names))\n",
    "        for name in product_names:\n",
    "            print(f\"Downloading {name}\")\n",
    "            group.create_task(download_one.fn(name, save_to_s3))\n",
    "            print(f\"{name} has been downloaded !\")\n",
    "\n",
    "@task(name='download_cadu_task')\n",
    "async def download_cadu(save_to_s3: bool, product_names: list):\n",
    "    # S3 bucket name\n",
    "    bucket_name = \"test-data\"\n",
    "    # Check if the s3 bucket already exist\n",
    "    if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "        print(f\"The bucket {bucket_name} already exists, removing the existing products from it ...\")\n",
    "        bucket_content = s3_client.list_objects(Bucket=bucket_name)\n",
    "        print(f\"Bucket {bucket_name} is clear !\")\n",
    "        # Check if the bucket is not empty\n",
    "        if 'Contents' in bucket_content:\n",
    "            all_s3_filenames = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "            # Remove the existing products from it\n",
    "            for file in all_s3_filenames:\n",
    "                s3_client.delete_object(Bucket=bucket_name, Key=file)\n",
    "    # Else create the bucket\n",
    "    else:\n",
    "        print(f\"The bucket {bucket_name} does not exist, creating the bucket {bucket_name} ...\")\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"The bucket {bucket_name} has beeen created !\")\n",
    "    # Remove all local files if they exist\n",
    "    print(\"Removing all local files if they exist ...\")\n",
    "    for name in product_names:\n",
    "        file = Path (local_download_dir) / name\n",
    "        if file.is_file():\n",
    "            file.unlink()\n",
    "    print(\"local download directory is clear !\")\n",
    "    await download_all.fn(save_to_s3, product_names)\n",
    "    await asyncio.sleep(1)\n",
    "    # If value save_to_s3 is True, download all the products and upload it on the bucket s3\n",
    "    if save_to_s3:\n",
    "        await asyncio.sleep(1)\n",
    "        all_s3_filenames = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "        for name in product_names:\n",
    "            is_missing = True\n",
    "            for filename in all_s3_filenames:\n",
    "                if name in filename:\n",
    "                    is_missing = False\n",
    "            if is_missing:\n",
    "                raise RuntimeError (f\"{name} is missing from the S3 bucket\")\n",
    "            print (f\"s3://{bucket_name}/{name} exists\")\n",
    "    # If value save_to_s3 is False, download all the products locally\n",
    "    else:\n",
    "        for name in product_names:\n",
    "            file = Path (local_download_dir) / name    \n",
    "            if not file.is_file():\n",
    "                raise RuntimeError (f\"{name} is missing locally\")\n",
    "            print (f\"{file} exists\")\n",
    "\n",
    "@flow(name='main_flow', log_prints=True)\n",
    "def working(save_to_s3: bool, date_start: str, date_end: str):\n",
    "    print(f\"Save to S3: {save_to_s3}.\")\n",
    "    product_names = search_cadu(date_start, date_end)\n",
    "    download_cadu(save_to_s3, product_names)\n",
    "\n",
    "working(False, start, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1eea57-d4c3-48a8-932b-61fff0df39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!prefect server start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddfd8e-e32c-4f01-97ff-35d80c4d6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_s3_filenames = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "all_s3_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aac532-45ac-4929-8c40-3223d5bc9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = s3_client.list_objects(Bucket=bucket_name)\n",
    "is_content = \"Contents\" in count\n",
    "is_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
