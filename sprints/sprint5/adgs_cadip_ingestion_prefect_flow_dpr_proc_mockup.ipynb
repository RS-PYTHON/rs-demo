{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6657010",
   "metadata": {},
   "source": [
    "Install the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b556c-b548-4428-8fe3-3cca45754287",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!(cd $RSPY_WHL_DIR && pip install rs_client_libraries-*.whl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7263f-b07d-4673-b6d3-301b035e76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket. \n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    "    region_name=os.environ[\"S3_REGION\"],\n",
    ")\n",
    "\n",
    "buckets = [\"tmp-download\", \"catalog-bucket\"]\n",
    "bucket_dir = \"stations\"\n",
    "bucket_url = f\"s3://{buckets[0]}/{bucket_dir}\"\n",
    "\n",
    "# If bucket is already created, clear all files in order to start fresh for each demo. \n",
    "for b in buckets:\n",
    "    if b in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "        if 'Contents' in s3_client.list_objects(Bucket=b):\n",
    "            objects = s3_client.list_objects(Bucket=b)['Contents']\n",
    "            for obj in objects:\n",
    "                # clear up the bucket\n",
    "                s3_client.delete_object(Bucket=b, Key=obj['Key'])\n",
    "    else:\n",
    "        s3_client.create_bucket(Bucket=b)\n",
    "for b in buckets:\n",
    "    print(f\"Is {b} empty ?: \", 'Contents' not in s3_client.list_objects(Bucket=b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472719ef",
   "metadata": {},
   "source": [
    "A bucket \"tmp-download\" is created for the purpose of this demo. Thus, the cadip and adgs prefect flows will be asking for the rs-server endpoints to download the files from CADIP and ADGS stations and to upload them to s3://auxiliary-files/stations/<station_name>\n",
    "After a succesfull upload to s3 bucket, the stac catalog is updated with the info related to that file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe25ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user's collection first (this has to be done on client side)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "\n",
    "@dataclass\n",
    "class Collection:\n",
    "    \"\"\"A collection for test purpose.\"\"\"\n",
    "\n",
    "    user: str\n",
    "    name: str\n",
    "\n",
    "    @property\n",
    "    def id_(self) -> str:\n",
    "        \"\"\"Returns the id.\"\"\"\n",
    "        return f\"{self.user}_{self.name}\"\n",
    "\n",
    "    @property\n",
    "    def properties(self):\n",
    "        \"\"\"Returns the properties.\"\"\"\n",
    "        return {\n",
    "            \"id\": self.name,\n",
    "            \"type\": \"Collection\",\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"items\",\n",
    "                    \"type\": \"application/geo+json\",\n",
    "                    \"href\": f\"http://localhost:8082/collections/{self.name}/items\",\n",
    "                },\n",
    "                {\"rel\": \"parent\", \"type\": \"application/json\", \"href\": \"http://localhost:8082/\"},\n",
    "                {\"rel\": \"root\", \"type\": \"application/json\", \"href\": \"http://localhost:8082/\"},\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"href\": f\"\"\"http://localhost:8082/collections/{self.name}\"\"\",\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"license\",\n",
    "                    \"href\": \"https://creativecommons.org/licenses/publicdomain/\",\n",
    "                    \"title\": \"public domain\",\n",
    "                },\n",
    "            ],\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\"bbox\": [[-94.6911621, 37.0332547, -94.402771, 37.1077651]]},\n",
    "                \"temporal\": {\"interval\": [[\"2000-02-01T00:00:00Z\", \"2000-02-12T00:00:00Z\"]]},\n",
    "            },\n",
    "            \"license\": \"public-domain\",\n",
    "            \"description\": \"Some description\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "        }\n",
    "    \n",
    "user = \"DemoUser\"\n",
    "mission = \"s1\"\n",
    "url_catalog = \"http://rs-server-catalog:8000\"\n",
    "\n",
    "# Create the collection for DemoUser\n",
    "collection_type = Collection(user, f\"{mission}_aux\")\n",
    "response = requests.post(url_catalog + f\"/catalog/{user}/collections\", json=collection_type.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed1ee5-56eb-4f46-95fa-a7cb2015fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from rs_workflows.common import (\n",
    "    PrefectFlowConfig,\n",
    "    download_flow,\n",
    ")\n",
    "\n",
    "def run_flow(user, url, url_catalog, station, mission, tmp_local_download, bucket_url, no_of_tasks, start_date, stop_date):\n",
    "    # start the prefect flow\n",
    "    download_flow(PrefectFlowConfig(user,\n",
    "                               url,\n",
    "                               url_catalog,\n",
    "                               station,\n",
    "                               mission,\n",
    "                               tmp_local_download,\n",
    "                               bucket_url,\n",
    "                               no_of_tasks,\n",
    "                               datetime.strptime(start_date, \"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                               datetime.strptime(stop_date, \"%Y-%m-%dT%H:%M:%SZ\"),                                   \n",
    "            )\n",
    ")\n",
    "\n",
    "user = \"DemoUser\"\n",
    "mission = \"s1\"\n",
    "stations = [\"CADIP\", \"ADGS\"]\n",
    "url = \"http://rs-server-{}:8000\"\n",
    "url_catalog = \"http://rs-server-catalog:8000\"\n",
    "tmp_local_download = \"/tmp/{}_tmp\"\n",
    "no_of_tasks = 1\n",
    "threads = []\n",
    "\n",
    "for station in stations:\n",
    "    run_flow(user,\n",
    "             url.format(station.lower()),\n",
    "             url_catalog,\n",
    "             station,\n",
    "             mission,\n",
    "             tmp_local_download.format(station),\n",
    "             bucket_url + f\"/{station}\",\n",
    "             no_of_tasks,\n",
    "             \"2014-01-01T12:00:00Z\",\n",
    "             \"2024-02-20T12:00:00Z\",\n",
    "             )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3487ca-12a4-418f-8e48-5d1938405c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions for a later use in generating the yaml file as input for the DPR mockup processor\n",
    "def gen_payload_inputs(s3_content):\n",
    "        yaml_content = []\n",
    "        cadu_id = 0\n",
    "        aux_id = 0\n",
    "        for file in s3_content:            \n",
    "            if \"AUX\" in file[\"Key\"]:\n",
    "                input_id = f\"AUX{aux_id}\"\n",
    "                aux_id += 1\n",
    "            else:    \n",
    "                input_id = f\"CADU{cadu_id}\"\n",
    "                cadu_id +=1\n",
    "            yaml_template = {\"id\": input_id, \"path\": file[\"Key\"], \"store_type\": \"zarr\", \"store_params\": {}}\n",
    "            yaml_content.append(yaml_template)\n",
    "        return yaml_content\n",
    "\n",
    "def gen_inputs_list(s3_content):\n",
    "    composer = []\n",
    "    cadu_id = 0\n",
    "    aux_id = 0\n",
    "    for input_cnt, file in enumerate(s3_content):\n",
    "        file_id = f'in{input_cnt}'\n",
    "        if \"AUX\" in file[\"Key\"]:\n",
    "            input_id = f\"AUX{aux_id}\"\n",
    "            aux_id += 1\n",
    "        else:    \n",
    "            input_id = f\"CADU{cadu_id}\"\n",
    "            cadu_id +=1\n",
    "        composer.append({file_id: input_id})\n",
    "    return composer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05825f0e",
   "metadata": {},
   "source": [
    "RSPY DPR Processor mockup demo:\n",
    "\n",
    "\n",
    "The DPRProcessor is a class that simulates processing part performed by eopf-cpm triggering module. The input of the processor it's a yaml config file with all the input files and expected outputs locations (local or s3).\n",
    "\n",
    "The implemented mockup performs the following actions:\n",
    "1. Check the validity of input yaml file (chunks/aux existance / naming convention)\n",
    "2. Downloads the zarr input from public s3 ovh based on product type required in payload yaml.\n",
    "3. Updates the .zattrs with our processor name (RSPY_DprMockupProcessor) and timestamp (if product is zipped, our processor updates zattrs inside .zip without extracting files)\n",
    "4. Computes the CRC of updated .zattrs\n",
    "5. Update product name VVV (as per EOPF-CPM PSD) with computed CRC, in order to call processor multiple times with same input and generated different outputs.\n",
    "6. Uploads the products to s3 server (minio for this demo).\n",
    "7. Removes the local downloaded products (if a flag is set).\n",
    "8. Retrieves the .zattrs into a serialisable format (dict) in order to upload catalog in the future step of our processing chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022e1bd-2e1f-4991-9e6d-e08fce8ef63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_payload = \"\"\"\n",
    "general_configuration:\n",
    "  logging:\n",
    "    level: DEBUG\n",
    "  triggering__validate_run: true\n",
    "  triggering__use_default_filename: true\n",
    "  triggering__use_basic_logging: true\n",
    "  triggering__load_default_logging: false\n",
    "breakpoints:\n",
    "workflow:\n",
    "- step: 1\n",
    "  active: true\n",
    "  module: rs.dpr.mockup # string corresponding to the python path of the module\n",
    "  processing_unit: DprMockupProcessor # EOProcessingUnit class name\n",
    "  name: DprMockupProcessor # identifier for the processing unit\n",
    "  inputs:\n",
    "  -\n",
    "  outputs:\n",
    "    out: outputs\n",
    "  parameters:\n",
    "    product_types: # List of EOPF product types we want to generate. In this example we simulate S1L0 processor that generates 4 products\n",
    "      - S1SSMOCN\n",
    "I/O:\n",
    "  inputs_products:\n",
    "   -\n",
    "  output_products:\n",
    "  - id: outputs\n",
    "    path: s3://test-processed-data/zarr/dpr_processor_output/ # output folder or S3 bucket\n",
    "    type: folder\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "dask_context: {}\n",
    "logging: {}\n",
    "config: {}\n",
    "\"\"\"\n",
    "\n",
    "# Update the yaml template with files downloaded from stations\n",
    "import yaml\n",
    "# Convert to yaml\n",
    "yaml_payload = yaml.safe_load(yaml_payload)\n",
    "# Update I/O and inputs\n",
    "print(s3_client.list_objects(Bucket=buckets[1]))\n",
    "yaml_payload[\"workflow\"][0]['inputs'] = gen_inputs_list(s3_client.list_objects(Bucket=buckets[1])[\"Contents\"])\n",
    "yaml_payload[\"I/O\"]['inputs_products'] = [item for item in gen_payload_inputs(s3_client.list_objects(Bucket=buckets[1])[\"Contents\"])]\n",
    "print(yaml.dump(yaml_payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b2ac0-e577-4498-8fb7-897f3e31bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket.\n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    "    region_name=os.environ[\"S3_REGION\"],\n",
    ")\n",
    "bucket_name = \"test-processed-data\"\n",
    "bucket_dir = \"zarr/dpr_processor_output\"\n",
    "bucket_url = f\"s3://{bucket_name}/{bucket_dir}\"\n",
    "\n",
    "# If bucket is already created, clear all files in order to start fresh for each demo. \n",
    "if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "    if 'Contents' in s3_client.list_objects(Bucket=bucket_name):\n",
    "        objects = s3_client.list_objects(Bucket=bucket_name)['Contents']\n",
    "        for obj in objects:\n",
    "            # clear up the bucket\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "else:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "print(\"Is bucket empty now ?: \", 'Contents' not in s3_client.list_objects(Bucket=bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661b61d-e3fe-4ad3-a855-d5fc67a37adc",
   "metadata": {},
   "source": [
    "Convert yaml to json in order to post it over HTTP and call the simulator webserver endpoint.\n",
    "The output of run() method is a list of all stac-comptabile .zattrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92871a-45d2-4779-ba5b-816932ddbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "dpr_simulator_endpoint = \"http://dpr-simulator:8000/run\" # rs-server host = the container name\n",
    "response = requests.post(dpr_simulator_endpoint, json=yaml_payload)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "for attr in response.json():\n",
    "    pp.pprint(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4ca11-5fcf-4626-8207-8bb3368e7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.list_objects(Bucket=bucket_name)['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee361b6-dc07-4499-a458-50ab6009b70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
