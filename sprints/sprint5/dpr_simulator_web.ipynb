{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2e1dc0-6eb0-4654-9626-419af4e34fc0",
   "metadata": {},
   "source": [
    "RSPY DPR Processor mockup demo:\n",
    "\n",
    "\n",
    "The DPRProcessor is a class that simulates processing part performed by eopf-cpm triggering module. The input of the processor it's a yaml config file with all the input files and expected outputs locations (local or s3).\n",
    "\n",
    "The implemented mockup performs the following actions:\n",
    "1. Check the validity of input yaml file (chunks/aux existance / naming convention)\n",
    "2. Downloads the zarr input from public s3 ovh based on product type required in payload yaml.\n",
    "3. Updates the .zattrs with our processor name (RSPY_DprMockupProcessor) and timestamp (if product is zipped, our processor updates zattrs inside .zip without extracting files)\n",
    "4. Computes the CRC of updated .zattrs\n",
    "5. Update product name VVV (as per EOPF-CPM PSD) with computed CRC, in order to call processor multiple times with same input and generated different outputs.\n",
    "6. Uploads the products to s3 server (minio for this demo).\n",
    "7. Removes the local downloaded products (if a flag is set).\n",
    "8. Retrieves the .zattrs into a serialisable format (dict) in order to upload catalog in the future step of our processing chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b556c-b548-4428-8fe3-3cca45754287",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "import requests\n",
    "import json\n",
    "import yaml\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685df198-48e8-4c95-9c65-8f756bb960c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_payload = \"\"\"\n",
    "general_configuration:\n",
    "  logging:\n",
    "    level: DEBUG\n",
    "  triggering__validate_run: true\n",
    "  triggering__use_default_filename: true\n",
    "  triggering__use_basic_logging: true\n",
    "  triggering__load_default_logging: false\n",
    "breakpoints:\n",
    "workflow:\n",
    "- step: 1\n",
    "  active: true\n",
    "  module: rs.dpr.mockup # string corresponding to the python path of the module\n",
    "  processing_unit: DprMockupProcessor # EOProcessingUnit class name\n",
    "  name: DprMockupProcessor # identifier for the processing unit\n",
    "  inputs:\n",
    "    in1: CADU1\n",
    "    in2: CADU2 # One CADU{N} entry by CADU chunk we want to pass as input. In this example we consider 2 chunks\n",
    "    in3: AUX1\n",
    "    in4: AUX2 # One AUX{N} entry by ADGS file we want to pass as input. In this example we consider 2 aux files\n",
    "  outputs:\n",
    "    out: outputs\n",
    "  parameters:\n",
    "    product_types: # List of EOPF product types we want to generate. In this example we simulate S1L0 processor that generates 4 products\n",
    "      - S1SSMOCN\n",
    "I/O:\n",
    "  inputs_products:\n",
    "  - id: CADU1\n",
    "    path: chunk/S1/S1A_20231121072204051312/DCS_04_S1A_20231121072204051312_ch1_DSDB_00023.raw\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "  - id: CADU2\n",
    "    path: chunk/S1/S1A_20231121072204051312/DCS_04_S1A_20231121072204051312_ch1_DSDB_00022.raw\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "  - id: AUX1\n",
    "    path: AUX/S1/S1A_OPER_AMV_ERRMAT_MPC__20201124T040009_V20000101T000000_20201123T131345.EOF.zip\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "  - id: AUX2\n",
    "    path: AUX/S1/S1A_AUX_PP2_V20190228T092500_G20220228T120000.SAFE.zip\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "  output_products:\n",
    "  - id: outputs\n",
    "    path: src/DPR/data/ # output folder or S3 bucket\n",
    "    type: folder\n",
    "    store_type: zarr\n",
    "    store_params: {}\n",
    "dask_context: {}\n",
    "logging: {}\n",
    "config: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db437f73-5442-49c1-8961-e7d8a498846d",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the s3 bucket using the minio console: http://127.0.0.1:9001/browser with:\n",
    "\n",
    "  * Username: _minio_\n",
    "  * Password: _Strong#Pass#1234_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b2ac0-e577-4498-8fb7-897f3e31bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket.\n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    "    region_name=os.environ[\"S3_REGION\"],\n",
    ")\n",
    "bucket_name = \"test-data\"\n",
    "bucket_dir = \"zarr/dpr_processor_output\"\n",
    "bucket_url = f\"s3://{bucket_name}/{bucket_dir}\"\n",
    "\n",
    "# If bucket is already created, clear all files in order to start fresh for each demo. \n",
    "if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "    if 'Contents' in s3_client.list_objects(Bucket=bucket_name):\n",
    "        objects = s3_client.list_objects(Bucket=bucket_name)['Contents']\n",
    "        for obj in objects:\n",
    "            # clear up the bucket\n",
    "            s3_client.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "else:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "print(\"Is bucket empty now ?: \", 'Contents' not in s3_client.list_objects(Bucket=bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661b61d-e3fe-4ad3-a855-d5fc67a37adc",
   "metadata": {},
   "source": [
    "Convert yaml to json in order to post it over HTTP and call the simulator webserver endpoint.\n",
    "The output of run() method is a list of all stac-comptabile .zattrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92871a-45d2-4779-ba5b-816932ddbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_data = yaml.safe_load(yaml_payload)\n",
    "json_data = json.dumps(yaml_data)\n",
    "\n",
    "dpr_simulator_endpoint = \"http://dpr-simulator:8000/run\" # rs-server host = the container name\n",
    "response = requests.post(dpr_simulator_endpoint, json=yaml_data)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "for attr in response.json():\n",
    "    pp.pprint(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4ca11-5fcf-4626-8207-8bb3368e7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.list_objects(Bucket=bucket_name)['Contents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
