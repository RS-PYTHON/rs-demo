{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b445d78-b8ce-459c-a34e-11af313fe1e3",
   "metadata": {},
   "source": [
    "# CADU endpoints demo\n",
    "\n",
    "In this demo we will call the rs-server CADU HTTP endpoints:\n",
    "\n",
    "  * List available CADU products\n",
    "  * Download some products into local storage and S3 bucket\n",
    "  * Monitor the download status from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f49d4-b62b-419a-8868-9072a40962f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "endpoint=\"http://rs-server:8000/cadip/CADIP/cadu\" # rs-server host = the container name\n",
    "start=\"2014-01-01T12:00:00.000Z\"\n",
    "stop=\"2023-12-30T12:00:00.000Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de3560-f30b-42b4-88ea-7ed93030fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a terminal, to list the available CADU products, we would use the curl command:\n",
    "!set -x && curl -X GET \"{endpoint}/list?start_date={start}&stop_date={stop}\" -H \"accept: application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e3adc-0ace-4343-8b47-3660aee4cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But let's do it in python so it's easier to parse results\n",
    "import requests\n",
    "import pprint \n",
    "\n",
    "# Call the \"list\" endpoint\n",
    "data = requests.get(f\"{endpoint}/list\", {\"start_date\": start, \"stop_date\": stop})\n",
    "assert data.status_code == 200\n",
    "\n",
    "# Get the returned products as (id,name) lists\n",
    "products = data.json()[\"CADIP\"]\n",
    "assert len(products) == 10\n",
    "\n",
    "# Print the first n products\n",
    "pprint.PrettyPrinter(indent=4).pprint(products[:3])\n",
    "print(\"...\")\n",
    "\n",
    "# Keep only the names\n",
    "product_names = [name for id, name in products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee015f-5e13-47bd-ad7d-81baff771a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"list\" endpoint has initialised the database with the products info.\n",
    "# Call the \"status\" endpoint to get the info from the products name.\n",
    "all_status = []\n",
    "for name in product_names:\n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    all_status.append (data.json())\n",
    "\n",
    "# Print the first n status\n",
    "pprint.PrettyPrinter(indent=4).pprint(all_status[:2])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03d397-7737-4d9d-ad11-235c4beed428",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the database using pgAdmin.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb527198-3896-4447-b758-ce54f2ae70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use boto3 to monitor the s3 bucket.\n",
    "# Note: the S3_ACCESSKEY, S3_SECRETKEY and S3_ENDPOINT are given in the docker-compose.yml file.\n",
    "!pip install boto3\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3_session = boto3.session.Session()\n",
    "s3_client = s3_session.client(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=os.environ[\"S3_ACCESSKEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_SECRETKEY\"],\n",
    "    endpoint_url=os.environ[\"S3_ENDPOINT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064243b-e760-4e63-8cee-6696614caf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name\n",
    "bucket_name = \"test-data\"\n",
    "\n",
    "# If the s3 bucket already exist, remove the existing products from it\n",
    "if bucket_name in [bucket[\"Name\"] for bucket in s3_client.list_buckets()[\"Buckets\"]]:\n",
    "    for name in product_names:\n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=name)\n",
    "\n",
    "# Else create the bucket\n",
    "else:\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "# The local download directory is passed as an environment variable\n",
    "local_download_dir = os.environ[\"RSPY_LOCAL_DOWNLOAD\"]\n",
    "\n",
    "# Remove all local files if they exist\n",
    "from pathlib import Path\n",
    "for name in product_names:\n",
    "    file = Path (local_download_dir) / name\n",
    "    if file.is_file():\n",
    "        file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549f2ca-cb6f-457f-a1cf-8b66e9426cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Call the CADIP endpoint to download one product in background \n",
    "# and upload it (optional) to the S3 bucket.\n",
    "async def download_one(name: str, save_to_s3: bool):\n",
    "\n",
    "    params = {\"name\": name, \"local\": local_download_dir}\n",
    "    # obs = the bucket URL, if requested\n",
    "    if save_to_s3:\n",
    "        params[\"obs\"] = f\"s3://{bucket_name}\"\n",
    "\n",
    "    data = requests.get(endpoint, params)\n",
    "    assert data.status_code == 200\n",
    "\n",
    "# In parallel, call the \"status\" endpoint to get and print the download status.\n",
    "async def print_status():\n",
    "\n",
    "    # Wait a second if the staus need to be passed \n",
    "    # from DONE to NOT_STARTED if we download several times.\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "    all_done = False\n",
    "    while not all_done: \n",
    "\n",
    "        # Count the number of products not started, in progres etc ...\n",
    "        all_status = {\"NOT_STARTED\": 0, \"IN_PROGRESS\": 0, \"FAILED\": 0, \"DONE\": 0}\n",
    "        for name in product_names:\n",
    "            \n",
    "            # Call the \"status\" endpoint\n",
    "            data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "            assert data.status_code == 200\n",
    "            all_status[(data.json())[\"status\"]] += 1\n",
    "\n",
    "        # Print result\n",
    "        print (\" / \".join ([f\"{status}:{count}\" for status, count in all_status.items()]))\n",
    "\n",
    "        if all_status[\"DONE\"] == len(product_names):\n",
    "            all_done = True\n",
    "        else:\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "# Call everything in parallel\n",
    "async def download_all(save_to_s3: bool):\n",
    "    async with asyncio.TaskGroup() as group:\n",
    "        group.create_task (print_status())\n",
    "        for name in product_names:\n",
    "            group.create_task(download_one (name, save_to_s3))\n",
    "\n",
    "print (\"Download everything to the local directory, not s3:\")\n",
    "await (download_all(save_to_s3=False))\n",
    "\n",
    "# Check that the local files exist. \n",
    "# Wait 1 second before that or sometimes it bugs.\n",
    "await asyncio.sleep(1)\n",
    "for name in product_names:\n",
    "    file = Path (local_download_dir) / name    \n",
    "    if not file.is_file():\n",
    "        raise RuntimeException (f\"{file} is missing locally\")\n",
    "    print (f\"{file} exists\")\n",
    "\n",
    "print (\"\\nDownload everything again, but this time upload to S3:\")\n",
    "await (download_all(save_to_s3=True))\n",
    "\n",
    "# This time the local files are not kept locally, \n",
    "# but they should be uploaded into the S3 bucket.\n",
    "await asyncio.sleep(1)\n",
    "all_s3_filenames = [key[\"Key\"] for key in s3_client.list_objects(Bucket=bucket_name)['Contents']]\n",
    "for name in product_names:    \n",
    "    if not name in all_s3_filenames:\n",
    "        raise RuntimeException (f\"{file} is missing from the S3 bucket\")\n",
    "    print (f\"s3://{bucket_name}/{name} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5462b2-f0c8-476c-ad56-1a814af85dbe",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**\n",
    "\n",
    "You can also monitor the s3 bucket using the minio console: http://127.0.0.1:9001/browser with:\n",
    "\n",
    "  * Username: _minio_\n",
    "  * Password: _Strong#Pass#1234_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d4303-1b6e-4472-9489-a028d318cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt_format = \"%Y-%m-%dT%H:%M:%S.%f\" # %z\n",
    "\n",
    "# Check timeliness by substracting download stop date - publishing date.\n",
    "# Call the \"status\" endpoint.\n",
    "print (\"Timeliness for:\")\n",
    "for name in product_names:    \n",
    "    data = requests.get(f\"{endpoint}/status\", {\"name\": name})\n",
    "    assert data.status_code == 200\n",
    "    values = data.json()\n",
    "    publication = datetime.strptime (values[\"available_at_station\"], dt_format)\n",
    "    stop = datetime.strptime (values[\"download_stop\"], dt_format)\n",
    "    timeliness = stop - publication\n",
    "    print (f\"  - {name}: {timeliness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c002f-6453-43ae-82db-a4e08b911c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
