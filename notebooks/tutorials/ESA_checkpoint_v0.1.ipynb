{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d2b49c-c2d0-4a21-a285-236fefb5faa8",
   "metadata": {},
   "source": [
    "<table align='right'><tr>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/EC_POS.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/ESA_logo_2020_Deep.png\" style=\"max-height:40px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/Copernicus_blue.png\" style=\"max-height:60px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/AIRBUS_Blue.png\" style=\"max-height:30px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/CS-GROUP.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaa622-5796-475b-8746-11a540613c64",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Reference System Python**</font> <br>\n",
    "**Copyright:** Copyright 2024 ESA <br>\n",
    "**License:** Apache License, Version 2.0 <br>\n",
    "**Authors:** Airbus, CS Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85eadd-c73b-4c45-b1d9-cefb26dee873",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>Copernicus Reference System Python tutorial for the ESA checkpoint 0.1</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf9681-d649-4b33-8c79-021e39e4dd1b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Data used\n",
    "\n",
    "In this notebook, we use simulated Auxip and Cadip data.\n",
    "\n",
    "<mark>TO BE DEFINED: will we use real data from real stations for the checkpoint ?</marked>\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know how to:\n",
    "* Use the RS-Client Python library.\n",
    "* Use it to:\n",
    "    * Search individual Auxip files and Cadip chunk files from the reception stations.\n",
    "    * Stage these files into the RS-Server STAC catalog.\n",
    "    * Use most of the STAC catalog functionalities.\n",
    "* Call Prefect flows to run parallel tasks to:\n",
    "    * Stage multiple Auxip and Cadip files at once.\n",
    "    * Run a simulated DPR processing on the staged files, and save results in the catalog.\n",
    "\n",
    "## Outline\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='TOC_TOP'></a>Contents\n",
    "\n",
    "</div>\n",
    "    \n",
    " 1. [Check your installation](#Check-your-installation) \n",
    " 1. [RsClient initialisation](#RsClient-initialisation)\n",
    " 1. [Call services manually](#Call-services-manually)\n",
    "     1. [Search Auxip and Cadip stations](#Search-Auxip-and-Cadip-stations)\n",
    "     1. [Stage Auxip and Cadip files](#Stage-Auxip-and-Cadip-files)\n",
    "     1. [Use the STAC catalog](#Use-the-STAC-catalog)\n",
    "     1. [Search Cadip sessions](#Search-Cadip-sessions)\n",
    " 1. [Prefect workflows](#Prefect-workflows)\n",
    "     1. [Workflow: initialisation](#Workflow:-initialisation)\n",
    "     1. [Workflow: stage Cadip files](#Workflow:-stage-Cadip-files)\n",
    "     1. [Workflow: stage Auxip files](#Workflow:-stage-Auxip-files)\n",
    "     1. [Workflow: DPR simulator](#Workflow:-DPR-simulator)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf81275-831c-48e8-bca0-1086434dcbc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Check your installation\n",
    "\n",
    "In this section, we will check that your Jupyter Notebook environment is correctly set.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d069303-fd50-49ea-9c47-9d23749806bd",
   "metadata": {},
   "source": [
    "### `rs-client-libraries` installation\n",
    "\n",
    "The `rs-client-libraries` Python library is the preferred way to access the RS-Server services from your environment. It is automatically installed in this notebook.\n",
    "\n",
    "**Note**: don't worry about these OpenTelemetry messages for now, they will be fixed in a later version:\n",
    "```\n",
    "Overriding of current TracerProvider is not allowed\n",
    "Attempting to instrument while already instrumented\n",
    "Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to ..., retrying in ...s\n",
    "Failed to export metrics to ..., error code: StatusCode.UNIMPLEMENTED\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69708f8f-84ca-45fc-9379-313f193c44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rs_client\n",
    "import rs_common\n",
    "import rs_workflows\n",
    "\n",
    "# Set logger level to info\n",
    "import logging\n",
    "rs_common.logging.Logging.level = logging.INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffce11f-fcf3-4514-a425-5ae2ecfca3b1",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cc16b-3dff-4909-9f72-2fb40834b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In local mode, all your services are running locally.\n",
    "# In hybrid or cluster mode, we use the services deployed on the RS-Server website.\n",
    "# This configuration is set in an environment variable.\n",
    "local_mode = (os.getenv(\"RSPY_LOCAL_MODE\") == \"1\")\n",
    "\n",
    "# In local mode, the service URLs are hardcoded in the docker-compose file\n",
    "if local_mode:\n",
    "    rs_server_href = None # not used\n",
    "    RSPY_PREFECT_URL = \"http://localhost:4200\"\n",
    "    RSPY_DPR_SIMU_URL = \"http://dpr-simulator:8000\"\n",
    "    print (f\"Auxip service: http://localhost:8001/docs\")\n",
    "    print (f\"CADIP service: http://localhost:8002/docs\")\n",
    "    print (f\"Catalog service: http://localhost:8003/api.html\")\n",
    "    print (f\"MinIO dashboard (object storage): http://localhost:9101 with user=minio password=Strong#Pass#1234\")\n",
    "    print (f\"Prefect dashboard (orchestrator): {RSPY_PREFECT_URL}\")\n",
    "    print (f\"Grafana dashboard (logs, traces, metrics): http://localhost:3000/explore\")\n",
    "\n",
    "# In hybrid or cluster mode, they are set in an environment variables\n",
    "else:\n",
    "    rs_server_href = os.environ[\"RSPY_WEBSITE\"]\n",
    "    RSPY_PREFECT_URL = os.environ['RSPY_PREFECT_URL']\n",
    "    RSPY_DPR_SIMU_URL = os.environ[\"RSPY_DPR_SIMU_URL\"]\n",
    "    print (f\"RS-Server website: {rs_server_href}\")\n",
    "    print (f\"Create an API key: {rs_server_href}/docs#/API-Key%20Manager/create_api_key_apikeymanager_auth_api_key_new_get\")\n",
    "    print (f\"Prefect dashboard (orchestrator): {RSPY_PREFECT_URL}\")\n",
    "    print (f\"Grafana dashboard (logs, traces, metrics): {os.environ['RSPY_GRAFANA_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23fe1d-8b50-488a-97d9-89af32a66ef5",
   "metadata": {},
   "source": [
    "### API key\n",
    "\n",
    "In hybrid and cluster mode, you need an API key to access the RS-Server services. \n",
    "\n",
    "You must create one from the link displayed from the previous cell, then enter it manually in the cell below. \n",
    "\n",
    "If you prefer to load it automatically in all your notebooks, you can: \n",
    "\n",
    "  * From your JupyterHub workspace, open the text file `~/.rspy` <mark>(TODO: name to be defined)</mark>\n",
    "  * Save your API key using this syntax:\n",
    "\n",
    "    ```bash\n",
    "    export RSPY_APIKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx # replace by your value\n",
    "    ```\n",
    "\n",
    "  * Save and close the file.\n",
    "  * <mark>TO BE CONFIRMED: Reload your JupyterHub session from Menu -> File -> Log Out / or just</mark>\n",
    "  * <mark>Reload this notebook kernel from Menu -> Kernel -> Restart Kernel.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32c2ed-f3ad-4adc-9642-bbbfe6985f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = os.getenv(\"RSPY_APIKEY\")\n",
    "if (not local_mode) and (not apikey):\n",
    "    import getpass\n",
    "    apikey = getpass.getpass(f\"Enter your API key:\")\n",
    "    os.environ[\"RSPY_APIKEY\"] = apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaed43-4218-41fc-a716-a487a6a6c322",
   "metadata": {},
   "source": [
    "### S3 buckets (object storage)\n",
    "\n",
    "The temporary S3 bucket is used to download the Auxip and Cadip files. \n",
    "\n",
    "When we publish these files into the STAC catalog, they are moved from the temporary into the final S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e7543-6ff4-4a0a-ae05-5e37d967f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use these bucket names that are deployed on the cluster. \n",
    "# RS-Server has read/write access to these buckets, but as an end-user, you won't manipulate them directly.\n",
    "TEMP_S3_BUCKET = \"rs-cluster-temp\"\n",
    "FINAL_S3_BUCKET = \"rs-cluster-catalog\"\n",
    "\n",
    "# Except in local mode, where we use a local MinIO object storage instance.\n",
    "# We need to manually create the buckets.\n",
    "if local_mode:\n",
    "    !pip install boto3\n",
    "    from resources.utils import create_s3_buckets\n",
    "    create_s3_buckets(TEMP_S3_BUCKET, FINAL_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862e220-e83a-4a9f-aa37-d384347ced05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## RsClient initialisation\n",
    "\n",
    "Initialise Python RsClient class instances to access the RS-Server services.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ca29b-1292-498b-94de-9191fc9458b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs_client.rs_client import RsClient\n",
    "from rs_common.config import ECadipStation\n",
    "\n",
    "# Init a generic RS-Client instance. Pass the:\n",
    "#   - RS-Server website URL\n",
    "#   - API key\n",
    "#   - ID of the owner of the STAC catalog collections.\n",
    "#     By default, this is the user login from the keycloak account, associated with the API key.\n",
    "#     Or, in local mode, this is the local system username.\n",
    "#     Else, your API Key must give you the rights to read/write on this catalog owner (see next cell).\n",
    "#   - Logger (optional, a default one can be used)\n",
    "generic_client = RsClient(rs_server_href, apikey, owner_id=None, logger=None)\n",
    "print(f\"Owner ID: {generic_client.owner_id!r}\")\n",
    "\n",
    "# From this generic instance, get an Auxip client instance\n",
    "auxip_client = generic_client.get_auxip_client()\n",
    "\n",
    "# Or get a Cadip client instance. Pass the cadip station.\n",
    "cadip_station = ECadipStation.CADIP\n",
    "cadip_client = generic_client.get_cadip_client(cadip_station)\n",
    "\n",
    "# Or get a Stac client to access the catalog\n",
    "stac_client = generic_client.get_stac_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8b5e2-7dfb-4674-8626-a2a9cc0235c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hybrid or cluster mode, show the user login and IAM roles from the keycloak account, \n",
    "# associated with the api key\n",
    "if not local_mode:\n",
    "    user_login = generic_client.apikey_user_login\n",
    "    iam_roles = \"\\n\".join(generic_client.apikey_iam_roles)\n",
    "    print(f\"API key user login: {user_login!r} \\nIAM roles: \\n{iam_roles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074e987-dda7-463c-9bd8-26e150cd870f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Call services manually\n",
    "\n",
    "In this section, we will see how to call manually these services: \n",
    "\n",
    "  * Search Auxip and Cadip reception stations for new files\n",
    "  * Stage these files and check the staging status\n",
    "  * STAC catalog services\n",
    "  * Search Cadip sessions\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068b294-713f-4b25-b6b2-1b7db9a9c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some initialisation\n",
    "from datetime import datetime\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "from rs_common.config import EDownloadStatus, EPlatform\n",
    "\n",
    "# Define a search interval\n",
    "start_date = datetime(2010, 1, 1, 12, 0, 0)\n",
    "stop_date = datetime(2024, 1, 1, 12, 0, 0)\n",
    "\n",
    "# Timeout in seconds for the endpoints\n",
    "TIMEOUT = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f05d1-1c37-4abd-acd2-118490c25bd9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Search Auxip and Cadip stations\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a113ddc-b3b1-4b35-98ab-86e586f9cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this using the Auxip client (to find Auxip files) \n",
    "# then the Cadip client (to find Cadip chunk files)\n",
    "for client in [auxip_client, cadip_client]:\n",
    "\n",
    "    # Call the service to search the reception stations for new files in the date interval.\n",
    "    files = client.search_stations(start_date, stop_date, TIMEOUT)\n",
    "    \n",
    "    file_count = len(files)\n",
    "    assert file_count, f\"We should have at least one {client.station_name} file\"\n",
    "    print (f\"Found {file_count} {client.station_name} files\\n\")\n",
    "\n",
    "    # Print the first file metadata. It is in the STAC format.\n",
    "    print(f\"First {client.station_name} file:\\n{json.dumps(files[0], indent=2)}\\n\")\n",
    "\n",
    "    # By default, the files are returned sorted by the most recent first (by creation date)\n",
    "    ids=\"\\n\".join([f\"{f['properties']['created']} - {f['id']}\" for f in files[:10]])\n",
    "    print(f\"Most recent {client.station_name} IDs and datetimes:\\n{ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd078a49-5768-4291-99cb-1885de815391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort by +/- any property, e.g. by creation date ascending = the oldest first\n",
    "for client in [auxip_client, cadip_client]:    \n",
    "    files = client.search_stations(start_date, stop_date, TIMEOUT, sortby=\"+created\")\n",
    "    ids=\"\\n\".join([f\"{f['properties']['created']} - {f['id']}\" for f in files[:10]])\n",
    "    print(f\"Oldest {client.station_name} IDs and datetimes:\\n{ids}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b28f2-1a06-47ca-967c-1dba7d99aff3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Stage Auxip and Cadip files\n",
    "\n",
    "When RS-Server stages a file, it means to:\n",
    "1. Copy (=download) it from the reception station into the temporary S3 bucket.\n",
    "1. Publish its metadata into the STAC catalog and move it from the temporary into the final S3 bucket.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9238c-13fe-4116-b87e-513903a18813",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_s3_files = []\n",
    "for client in [auxip_client, cadip_client]:\n",
    "\n",
    "    # When searching stations, we can also limit the number of returned results.\n",
    "    # For this example, let's keep only one file.\n",
    "    files = client.search_stations(start_date, stop_date, TIMEOUT, limit=1)\n",
    "    assert len(files) == 1\n",
    "\n",
    "    # We stage by filename = the file ID\n",
    "    first_filename = files[0][\"id\"]\n",
    "\n",
    "    # We must give a temporary S3 bucket path where to copy the file from the station.\n",
    "    # Use our API key username so avoid conflicts with other users.\n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{client.apikey_user_login}/{client.station_name}\"\n",
    "    temp_s3_files.append (f\"{s3_path}/{first_filename}\") # save it for later\n",
    "\n",
    "    # We can also download the file locally to the server, but this is useful only in local mode\n",
    "    local_path = None\n",
    "\n",
    "    # Call the staging service\n",
    "    client.staging(first_filename, TIMEOUT, s3_path=s3_path, tmp_download_path=local_path)\n",
    "\n",
    "    # Then we can check when the staging has finished by calling the check status service\n",
    "    while True:\n",
    "        status = client.staging_status(first_filename, TIMEOUT)\n",
    "        print (f\"Staging status for {first_filename!r}: {status.value}\")\n",
    "        if status in [EDownloadStatus.DONE, EDownloadStatus.FAILED]:\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        sleep(1)\n",
    "\n",
    "    # WARNING: the file is copied into the temporary S3 bucket but is not yet published into the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba922daf-0aad-4cf8-8755-cd87e40e8942",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Use the STAC catalog\n",
    "\n",
    "The SpatioTemporal Asset Catalog (STAC) family of specifications aim to standardize the way geospatial asset metadata is structured and queried. \n",
    "\n",
    "A 'spatiotemporal asset' is any file that represents information about the Earth captured in a certain space and time. \n",
    "\n",
    "For more information, see: https://github.com/radiantearth/stac-api-spec/tree/main\n",
    "\n",
    "In this section, we will see how to use most of the RS-Server STAC catalog functionalities.\n",
    "\n",
    "<mark>TODO: this part must be completed and replaced by the StacClient</mark>\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f4a28-49ac-4772-836f-2266d9920324",
   "metadata": {},
   "source": [
    "#### Create a new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5d52f-9206-4505-a127-5bc87bcb082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "COLLECTION = \"my_collection\"\n",
    "\n",
    "collection = {\n",
    "            \"id\": COLLECTION,\n",
    "            \"type\": \"Collection\",\n",
    "            \"description\": \"This is my collection description\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"owner\": stac_client.owner_id\n",
    "        }\n",
    "\n",
    "# Clean the existing collection, if any\n",
    "requests.delete(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{COLLECTION}\", **stac_client.apikey_headers, stream=True)\n",
    "\n",
    "# Create it from new\n",
    "response = requests.post(f\"{stac_client.href_catalog}/catalog/collections\", json=collection, **stac_client.apikey_headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# See my personal catalog information\n",
    "response = requests.get(f\"{stac_client.href_catalog}/catalog/catalogs/{stac_client.owner_id}\", **stac_client.apikey_headers)\n",
    "response.raise_for_status()\n",
    "print (f\"My catalog information:\\n{json.dumps (json.loads (response.content), indent=2)}\")\n",
    "\n",
    "# See my collection information\n",
    "response = requests.get(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{COLLECTION}\", **stac_client.apikey_headers)\n",
    "response.raise_for_status()\n",
    "print (f\"\\nMy collection information:\\n{json.dumps (json.loads (response.content), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9474a-be39-4983-9e27-d43046859ade",
   "metadata": {},
   "source": [
    "#### Add new items to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb97df-6b77-4ac9-b42f-645d02ae964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add one Auxip and one Cadip file that were staged from the previous steps\n",
    "for temp_s3_file in temp_s3_files:\n",
    "\n",
    "    # Let's use STAC item ID = filename\n",
    "    print(f\"\\nAdd catalog item from: {temp_s3_file!r}\")\n",
    "    item_id = os.path.basename(temp_s3_file)\n",
    "    \n",
    "    item = {\n",
    "                \"id\": item_id,\n",
    "                \"bbox\": [-94.6334839, 37.0332547, -94.6005249, 37.0595608],\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": [\n",
    "                        [\n",
    "                            [-94.6334839, 37.0595608],\n",
    "                            [-94.6334839, 37.0332547],\n",
    "                            [-94.6005249, 37.0332547],\n",
    "                            [-94.6005249, 37.0595608],\n",
    "                            [-94.6334839, 37.0595608],\n",
    "                        ]\n",
    "                    ],\n",
    "                },\n",
    "                \"collection\": COLLECTION,\n",
    "                \"properties\": {\n",
    "                    \"gsd\": 0.5971642834779395,\n",
    "                    \"width\": 2500,\n",
    "                    \"height\": 2500,\n",
    "                    \"datetime\": \"2000-02-02T00:00:00Z\",\n",
    "                    \"proj:epsg\": 3857,\n",
    "                    \"orientation\": \"nadir\",\n",
    "                    \"owner_id\": stac_client.owner_id,\n",
    "                },\n",
    "                \"stac_extensions\": [],\n",
    "                \"assets\": {\n",
    "                    \"file\": {\n",
    "                        \"href\": temp_s3_file,\n",
    "                        \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                        \"title\": \"NOAA STORM COG\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "\n",
    "    # WARNING: after this, the staged file is moved from the temporary into the final bucket,\n",
    "    # so this cell can be run only once, or you'll need to stage the files again from the previous section.\n",
    "    \n",
    "    response = requests.post(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{COLLECTION}/items\", json=item, **stac_client.apikey_headers)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    response = requests.get(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{COLLECTION}/items/{item_id}\", **stac_client.apikey_headers)\n",
    "    response.raise_for_status()\n",
    "    inserted_item = json.loads (response.content)\n",
    "    print (f\"Saved item in the catalog:\\n{json.dumps (inserted_item, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15f3d0-e3ae-4ab4-9fe6-243615b425bc",
   "metadata": {},
   "source": [
    "<mark>TODO: demonstrate the other catalog endpoints. Demonstrate the create_cql2_filter ? (used below)</mark>\n",
    "\n",
    "<mark>print the links to download the files (requests.get(stac_client.href_catalog + f\"/catalog/collections/{owner_id}:{mission}_aux/items/{feature['id']}/download/file\")</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc7a02-5a20-49ab-8081-7c1d0caec45e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Search Cadip sessions\n",
    "\n",
    "All Cadip chunk files are attached to a single Cadip session.\n",
    "\n",
    "We can search Cadip sessions by parameters, or find information about a specific session ID.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81869f26-c21e-4359-a959-29534798e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search cadip sessions by date interval and platforms\n",
    "platforms = [EPlatform.S1A, EPlatform.S2B]\n",
    "sessions = cadip_client.search_sessions(TIMEOUT, start_date=start_date, stop_date=stop_date, platforms=platforms)\n",
    "\n",
    "session_count = len(sessions)\n",
    "assert session_count, \"We should have at least one Cadip session\"\n",
    "print (f\"Found {session_count} Cadip sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45253518-999f-4f2e-8759-d9a7f39a71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first cadip session metadata. It is in the STAC format.\n",
    "print(f\"First Cadip session:\\n{json.dumps(sessions[0], indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c7920-3942-4156-88a3-f503ddf7230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the Cadip session IDs\n",
    "ids=[s[\"id\"] for s in sessions]\n",
    "print_ids=\"\\n\".join(ids)\n",
    "print(f\"Cadip sessions IDs:\\n{print_ids}\")\n",
    "\n",
    "# Save the first session ID for later\n",
    "first_cadip_session = ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711fe8e-0cb6-4e9a-b0bd-42f76c97c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also search Cadip sessions by specific session IDs,\n",
    "# e.g. get information for the cadip sessions #2 and #3\n",
    "search_ids=ids[1:3]\n",
    "search_sessions = cadip_client.search_sessions(TIMEOUT, session_ids=search_ids)\n",
    "print(f\"Cadip sessions information:\\n{json.dumps(search_sessions, indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541a8bc-0786-48ed-8f15-1a1461d98cb8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Exercises: call services manually\n",
    "\n",
    "<mark>TODO: stage and push other files to the catalog, using another owner on which you have permissions. Print other session info.</mark>\n",
    "\n",
    "NOTE: also use the endpoints from the SWAGGER\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73bfc1-d12b-4c6a-bf97-766e72b6fc66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Prefect workflows\n",
    "[Back to top](#Contents)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9ae81-a627-4887-b462-97979004cdab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: initialisation\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d953a-a0e0-4f87-94b3-55ba9ee7ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# The workflow will stage all files between a date interval.\n",
    "# In this example, we will stage all the files of the first Cadip session.\n",
    "# We extract the mission and date from the session ID: <mission>_YYYYmmdd<other_info>\n",
    "session_id = first_cadip_session\n",
    "print(f\"First Cadip session ID: {session_id!r}\")\n",
    "mission, date_and_other = session_id.split(\"_\")\n",
    "date = datetime.strptime (date_and_other[:8], \"%Y%m%d\")\n",
    "start_datetime = date # start from midnight\n",
    "stop_datetime = date + timedelta(days=1) # midnight the day after\n",
    "\n",
    "print(f\"Mission: {mission!r}\")\n",
    "print(f\"Date interval: '{start_datetime} -> {stop_datetime}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a9bd0-ac99-4829-9094-c323f052505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a prerequisite, we must create manually the Auxip and Cadip \n",
    "# collections in the catalog, if they don't already exist.\n",
    "from rs_workflows import staging\n",
    "\n",
    "for client in [auxip_client, cadip_client]:\n",
    "    collection_name = staging.create_collection_name(mission, client.station_name)\n",
    "\n",
    "    # Save the collection name for later\n",
    "    if client == auxip_client:\n",
    "        auxip_collection = collection_name\n",
    "    else:\n",
    "        cadip_collection = collection_name\n",
    "\n",
    "    # Try to get collection information    \n",
    "    response = requests.get(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{collection_name}\", **stac_client.apikey_headers)\n",
    "\n",
    "    # If it's a 404, this means that the collection doesn't exist, so create it\n",
    "    if response.status_code == 404:\n",
    "        \n",
    "        collection = {\n",
    "            \"id\": collection_name,\n",
    "            \"type\": \"Collection\",\n",
    "            \"description\": \"This is my collection description\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"owner\": stac_client.owner_id\n",
    "        }\n",
    "        print(f\"Create collection: {collection_name!r}\")\n",
    "        response = requests.post(f\"{stac_client.href_catalog}/catalog/collections\", json=collection, **stac_client.apikey_headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Collection already exists: {collection_name!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8958f1f-737a-4d1f-96c0-02756d1926af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Cadip files\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422d974-9e50-44ec-a7f9-66fc6f94f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for now, in hybrid mode, it's the localhost URL that is used\n",
    "print(f\"\\nView Prefect flow runs from: {RSPY_PREFECT_URL}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432dfc6-aca9-46f4-8a0d-6749b11786bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tasks to be run in parallel\n",
    "MAX_WORKERS = 15\n",
    "\n",
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    cadip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{cadip_client.owner_id}/{cadip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_datetime,\n",
    "    stop_datetime=stop_datetime,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c67f70-38ae-4e82-b0b4-f52ab58bc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs_workflows import s1_l0\n",
    "\n",
    "# We can use this filter to find the staged Cadip files for this session ID\n",
    "query = s1_l0.create_cql2_filter({\"collection\": f\"{cadip_client.owner_id}_{cadip_collection}\", \"cadip:session_id\": session_id})\n",
    "response = requests.post(f\"{stac_client.href_catalog}/catalog/search\", json=query, **stac_client.apikey_headers)\n",
    "\n",
    "response.raise_for_status()\n",
    "files = json.loads (response.content)\n",
    "print (f\"\\n{files['context']['returned']} Cadip files are staged for session ID: {session_id!r}.\")\n",
    "print (f\"First one:\\n{json.dumps (files['features'][0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019b40-326c-4e3f-a9bd-33af6eaa26ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Auxip files\n",
    "\n",
    "We need to pass Auxip files to the DPR processing. They must be staged into the catalog.\n",
    "\n",
    "As for now, the DPR processing is only a simulation, we can pass any Auxip files.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f05d5-3ffc-4b7e-8d0d-cf2bafcfc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the 3 most recent files between today and any old date\n",
    "files = auxip_client.search_stations(\n",
    "    datetime(year=1970, month=1, day=1), \n",
    "    datetime.today(), \n",
    "    TIMEOUT, \n",
    "    sortby=\"-created\",\n",
    "    limit=None) # NOTE: for now \"limit\" is not working well with \"sortby\" so don't use it\n",
    "\n",
    "# Only keep the first 3 files\n",
    "files = files[:3]\n",
    "\n",
    "# Save the IDs = the filenames\n",
    "auxip_files = [f[\"id\"] for f in files]\n",
    "print_ids = \"\\n\".join(auxip_files)\n",
    "print(f\"Auxip files: \\n{print_ids}\")\n",
    "\n",
    "# Save the min and max dates for these 3 files\n",
    "dates = [datetime.strptime (f[\"properties\"][\"created\"], \"%Y-%m-%dT%H:%M:%S.%fZ\") for f in files]\n",
    "start_datetime = min(dates) - timedelta(seconds=1) # remove 1 second because the interval is exclusive\n",
    "stop_datetime = max(dates) + timedelta(seconds=1) # add 1 second\n",
    "print(f\"\\nDate interval: '{start_datetime} -> {stop_datetime}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa6a29-69a9-4d23-b290-3475e80e5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    auxip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{auxip_client.owner_id}/{auxip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_datetime,\n",
    "    stop_datetime=stop_datetime,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4841f-3798-4324-ade9-cd63ca76ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have 3 items in the Auxip collection\n",
    "response = requests.get(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{auxip_collection}/items/\", **stac_client.apikey_headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "content = json.loads(response.content)\n",
    "staged_auxip = [feature[\"id\"] for feature in content[\"features\"]]\n",
    "print_ids = \"\\n\".join(staged_auxip)\n",
    "print (f\"{content['context']['returned']} Auxip files are staged:\\n{print_ids}\")\n",
    "\n",
    "# Check that the expected Auxip files were staged\n",
    "for file in auxip_files:\n",
    "    assert file in staged_auxip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19670b80-2a72-4e8d-b6c1-ff5b8c7f576e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: DPR simulator\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41502ab-dcfb-4d8f-a4c3-e2a72e21e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(s1_l0)\n",
    "session_id=\"S1A_20200105072204051312\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aaa8d-e0e8-468c-bbe2-d904eef0ca3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rs_workflows import s1_l0\n",
    "\n",
    "# The product types to process can be any of these 4 values\n",
    "product_types = [\"S1SEWRAW\", \"S1SIWRAW\", \"S1SSMRAW\", \"S1SWVRAW\"]\n",
    "\n",
    "# DPR workflow configuration\n",
    "config = s1_l0.PrefectS1L0FlowConfig(\n",
    "    stac_client,\n",
    "    RSPY_DPR_SIMU_URL,\n",
    "    mission,\n",
    "    session_id,\n",
    "    product_types,\n",
    "    auxip_files,\n",
    "    s3_path = f\"s3://{FINAL_S3_BUCKET}/{client.owner_id}/DPR_S1L0\",\n",
    "    temp_s3_path = f\"s3://{TEMP_S3_BUCKET}/{client.owner_id}/DPR_S1L0\",\n",
    ")\n",
    "\n",
    "# Start the prefect flow\n",
    "s1_l0.s1_l0_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912853c-87e9-464b-9611-91a79f78ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DPR collection name is hardcoded in the workflow\n",
    "dpr_collection = f\"{mission}_dpr\"\n",
    "\n",
    "# We should have one item in the DPR collection for each product type\n",
    "response = requests.get(f\"{stac_client.href_catalog}/catalog/collections/{stac_client.owner_id}:{dpr_collection}/items/\", **stac_client.apikey_headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "content = json.loads(response.content)\n",
    "features = content[\"features\"]\n",
    "ids = \"\\n\".join([feature[\"id\"] for feature in features])\n",
    "\n",
    "print (f\"{len(features)} DPR items in the catalog:\\n{ids}\")\n",
    "print (f\"\\nFirst item:\\n{json.dumps(features[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b809785-085f-4f8a-8717-d68235b5b9d2",
   "metadata": {},
   "source": [
    "<mark>TODO: show DPR items from the catalog</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43fa8c-4558-41d6-92cf-70ceb7764e79",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"https://github.com/RS-PYTHON\" target=\"_blank\">View on GitLab</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c978a-df59-496a-806d-89b618dcbcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
