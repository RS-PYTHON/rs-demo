{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d2b49c-c2d0-4a21-a285-236fefb5faa8",
   "metadata": {},
   "source": [
    "<table align='right'><tr>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/EC_POS.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/ESA_logo_2020_Deep.png\" style=\"max-height:40px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/Copernicus_blue.png\" style=\"max-height:60px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/AIRBUS_Blue.png\" style=\"max-height:30px;width:auto;\"/></td>\n",
    "<td style=\"padding:10px\"><img src=\"resources/img/CS-GROUP.png\" style=\"max-height:50px;width:auto;\"/></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeaa622-5796-475b-8746-11a540613c64",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Reference System Python**</font> <br>\n",
    "**Copyright:** Copyright 2024 ESA <br>\n",
    "**License:** Apache License, Version 2.0 <br>\n",
    "**Authors:** Airbus, CS Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85eadd-c73b-4c45-b1d9-cefb26dee873",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>Copernicus Reference System Python tutorial for the ESA checkpoint 0.1</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf9681-d649-4b33-8c79-021e39e4dd1b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Links\n",
    "\n",
    "* GitHub: https://github.com/RS-PYTHON\n",
    "* Documentation: https://rs-python.github.io/rs-documentation/\n",
    "\n",
    "## Data used\n",
    "\n",
    "In this notebook, we use simulated Auxip and Cadip data.\n",
    "\n",
    "<mark>TO BE DEFINED: will we use real data from real stations for the checkpoint ?</marked>\n",
    "\n",
    "## Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know how to:\n",
    "* Use the RS-Client Python library.\n",
    "* Use it to:\n",
    "    * Search individual Auxip files and Cadip chunk files from the Auxip and Cadip simulators.\n",
    "    * Stage these files into the RS-Server STAC catalog.\n",
    "    * Use most of the STAC catalog functionalities.\n",
    "* Call Prefect flows to run parallel tasks to:\n",
    "    * Stage multiple Auxip and Cadip files at once.\n",
    "    * Run a simulated DPR processing on the staged files, and save results in the catalog.\n",
    "\n",
    "## Outline\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Contents\n",
    "\n",
    "</div>\n",
    "    \n",
    "1. [Check your installation](#Check-your-installation) \n",
    "1. [RsClient initialisation](#RsClient-initialisation)\n",
    "1. [Call services manually](#Call-services-manually)\n",
    "    1. [Search Auxip and Cadip stations](#Search-Auxip-and-Cadip-stations)\n",
    "    1. [Stage Auxip and Cadip files](#Stage-Auxip-and-Cadip-files)\n",
    "    1. [Use the STAC catalog](#Use-the-STAC-catalog)\n",
    "    1. [Search Cadip sessions](#Search-Cadip-sessions)\n",
    "    1. [Exercises](#Exercises:-call-services-manually)\n",
    " 1. [Prefect workflows](#Prefect-workflows)\n",
    "     1. [Initialisation](#Workflow:-initialisation)\n",
    "     1. [Stage Cadip chunk files](#Workflow:-stage-Cadip-chunk-files)\n",
    "     1. [Stage Auxip files](#Workflow:-stage-Auxip-files)\n",
    "     1. [DPR simulator](#Workflow:-DPR-simulator)\n",
    "     1. [Exercises](#Exercises:-workflows)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf81275-831c-48e8-bca0-1086434dcbc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Check your installation\n",
    "\n",
    "In this section, we will check that your Jupyter Notebook environment is correctly set.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d069303-fd50-49ea-9c47-9d23749806bd",
   "metadata": {},
   "source": [
    "### `rs-client-libraries` installation\n",
    "\n",
    "The `rs-client-libraries` Python library is the preferred way to access the RS-Server services from your environment. It is automatically installed in this notebook.\n",
    "\n",
    "**Note**: don't worry about these OpenTelemetry messages for now, they will be fixed in a later version:\n",
    "```\n",
    "Overriding of current TracerProvider is not allowed\n",
    "Attempting to instrument while already instrumented\n",
    "Transient error StatusCode.UNAVAILABLE encountered while exporting metrics to ..., retrying in ...s\n",
    "Failed to export metrics to ..., error code: StatusCode.UNIMPLEMENTED\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69708f8f-84ca-45fc-9379-313f193c44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rs_client\n",
    "import rs_common\n",
    "import rs_workflows\n",
    "\n",
    "# Set logger level to info\n",
    "import logging\n",
    "rs_common.logging.Logging.level = logging.INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffce11f-fcf3-4514-a425-5ae2ecfca3b1",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cc16b-3dff-4909-9f72-2fb40834b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# In local mode, all your services are running locally.\n",
    "# In hybrid or cluster mode, we use the services deployed on the RS-Server website.\n",
    "# This configuration is set in an environment variable.\n",
    "local_mode = (os.getenv(\"RSPY_LOCAL_MODE\") == \"1\")\n",
    "\n",
    "# In local mode, the service URLs are hardcoded in the docker-compose file\n",
    "if local_mode:\n",
    "    rs_server_href = None # not used\n",
    "    RSPY_HOST_AUXIP = \"http://localhost:8001/docs\"\n",
    "    RSPY_HOST_CADIP = \"http://localhost:8002/docs\"\n",
    "    RSPY_HOST_CATALOG = \"http://localhost:8003/api.html\"\n",
    "    RSPY_PREFECT_URL = \"http://localhost:4200\"\n",
    "    RSPY_DPR_SIMU_URL = \"http://dpr-simulator:8000\"\n",
    "    print (f\"Auxip service: {RSPY_HOST_AUXIP}\")\n",
    "    print (f\"CADIP service: {RSPY_HOST_CADIP}\")\n",
    "    print (f\"Catalog service: {RSPY_HOST_CATALOG}\")\n",
    "    print (f\"MinIO dashboard (object storage): http://localhost:9101 with user=minio password=Strong#Pass#1234\")\n",
    "    print (f\"Prefect dashboard (orchestrator): {RSPY_PREFECT_URL}\")\n",
    "    print (f\"Grafana dashboard (logs, traces, metrics): http://localhost:3000/explore\")\n",
    "\n",
    "# In hybrid or cluster mode, they are set in an environment variables\n",
    "else:\n",
    "    rs_server_href = os.environ[\"RSPY_WEBSITE\"]\n",
    "    RSPY_PREFECT_URL = os.environ['RSPY_PREFECT_URL']\n",
    "    RSPY_DPR_SIMU_URL = os.environ[\"RSPY_DPR_SIMU_URL\"]\n",
    "    print (f\"RS-Server website: {rs_server_href}\")\n",
    "    print (f\"Create an API key: {rs_server_href}/docs#/API-Key%20Manager/create_api_key_apikeymanager_auth_api_key_new_get\")\n",
    "    print (f\"Prefect dashboard (orchestrator): {RSPY_PREFECT_URL}\")\n",
    "    print (f\"Grafana dashboard (logs, traces, metrics): {os.environ['RSPY_GRAFANA_URL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23fe1d-8b50-488a-97d9-89af32a66ef5",
   "metadata": {},
   "source": [
    "### API key\n",
    "\n",
    "In hybrid and cluster mode, you need an API key to access the RS-Server services. \n",
    "\n",
    "You must create one from the link displayed from the previous cell, see: <https://rs-python.github.io/rs-documentation/rs-server/docs/doc/users/oauth2_apikey_manager>\n",
    "\n",
    "Then enter it manually in the cell below. \n",
    "\n",
    "It is easier to save it into your `~/.env` file so it is loaded automatically by your notebooks. To do so, run this line from your JupyterHub terminal, then restart this notebook kernel: \n",
    "\n",
    "```shell\n",
    "# Replace by your value\n",
    "echo \"export RSPY_APIKEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" >> ~/.env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32c2ed-f3ad-4adc-9642-bbbfe6985f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = os.getenv(\"RSPY_APIKEY\")\n",
    "if (not local_mode) and (not apikey):\n",
    "    import getpass\n",
    "    apikey = getpass.getpass(f\"Enter your API key:\")\n",
    "    os.environ[\"RSPY_APIKEY\"] = apikey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaed43-4218-41fc-a716-a487a6a6c322",
   "metadata": {},
   "source": [
    "### S3 buckets (object storage)\n",
    "\n",
    "The temporary S3 bucket is used to download the Auxip and Cadip files. \n",
    "\n",
    "When we publish these files into the STAC catalog, they are moved from the temporary into the final S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e7543-6ff4-4a0a-ae05-5e37d967f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use these bucket names that are deployed on the cluster. \n",
    "# RS-Server has read/write access to these buckets, but as an end-user, you won't manipulate them directly.\n",
    "TEMP_S3_BUCKET = \"rs-cluster-temp\"\n",
    "FINAL_S3_BUCKET = \"rs-cluster-catalog\"\n",
    "\n",
    "# Except in local mode, where we use a local MinIO object storage instance.\n",
    "# We need to manually create the buckets.\n",
    "if local_mode:\n",
    "    !pip install boto3\n",
    "    from resources.utils import create_s3_buckets\n",
    "    create_s3_buckets(TEMP_S3_BUCKET, FINAL_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862e220-e83a-4a9f-aa37-d384347ced05",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## RsClient initialisation\n",
    "\n",
    "Initialise Python RsClient class instances to access the RS-Server services.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ca29b-1292-498b-94de-9191fc9458b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rs_client.rs_client import RsClient\n",
    "from rs_common.config import ECadipStation\n",
    "\n",
    "# Init a generic RS-Client instance. Pass the:\n",
    "#   - RS-Server website URL\n",
    "#   - API key\n",
    "#   - ID of the owner of the STAC catalog collections.\n",
    "#     By default, this is the user login from the keycloak account, associated to the API key.\n",
    "#     Or, in local mode, this is the local system username.\n",
    "#     Else, your API Key must give you the rights to read/write on this catalog owner (see next cell).\n",
    "#   - Logger (optional, a default one can be used)\n",
    "generic_client = RsClient(rs_server_href, apikey, owner_id=None, logger=None)\n",
    "print(f\"STAC catalog owner: {generic_client.owner_id!r}\")\n",
    "\n",
    "# From this generic instance, get an Auxip client instance\n",
    "auxip_client = generic_client.get_auxip_client()\n",
    "\n",
    "# Or get a Cadip client instance. Pass the cadip station.\n",
    "cadip_station = ECadipStation.CADIP # you can also have: INS, MPS, MTI, NSG, SGS\n",
    "cadip_client = generic_client.get_cadip_client(cadip_station)\n",
    "\n",
    "# Or get a Stac client to access the catalog\n",
    "stac_client = generic_client.get_stac_client()\n",
    "\n",
    "print(\"\\nValidate that our catalog is valid to the STAC format...\")\n",
    "stac_client.validate_all()\n",
    "\n",
    "print(\"\\nDisplay the Stac catalog as a treeview in notebook:\")\n",
    "display(stac_client)\n",
    "\n",
    "print(\"\\nOr just display all its contents at once:\")\n",
    "print(json.dumps(stac_client.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8b5e2-7dfb-4674-8626-a2a9cc0235c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hybrid or cluster mode, show information from the keycloak account, associated to the api key\n",
    "if not local_mode:\n",
    "\n",
    "    # = keycloak account user login\n",
    "    print(f\"API key user login: {generic_client.apikey_user_login!r}\")\n",
    "\n",
    "    # Print the IAM (Identity and Access Management) roles\n",
    "    # For this tutorial, you must have: \n",
    "    #   - read/download access for Adgs (=Auxip) = \"rs_adgs_<read|download>\"\n",
    "    #   - read/download access to the Cadip station you passed on the above cell = \"rs_cadip_<station>_<read|download>\"\n",
    "    #   - (optional) read/write/download access to STAC catalog collections from other owners = \"rs_catalog_<owner_id>:<collection|*>_<read|write|download>\"\n",
    "    #     (you always have all access to your own collections with owner_id=apikey_user_login as printed above)\n",
    "    iam_roles = \"\\n\".join (sorted (generic_client.apikey_iam_roles))\n",
    "    print(f\"\\nAPI key IAM roles: \\n{iam_roles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074e987-dda7-463c-9bd8-26e150cd870f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Call services manually\n",
    "\n",
    "In this section, we will see how to call manually these services: \n",
    "\n",
    "  * Search Auxip and Cadip reception stations for new files\n",
    "  * Stage these files and check the staging status\n",
    "  * STAC catalog services\n",
    "  * Search Cadip sessions\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068b294-713f-4b25-b6b2-1b7db9a9c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some initialisation\n",
    "from datetime import datetime\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "from rs_common.config import EDownloadStatus, EPlatform\n",
    "\n",
    "# Define a search interval\n",
    "start_date = datetime(2010, 1, 1, 12, 0, 0)\n",
    "stop_date = datetime(2024, 1, 1, 12, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f05d1-1c37-4abd-acd2-118490c25bd9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Search Auxip and Cadip stations\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a113ddc-b3b1-4b35-98ab-86e586f9cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this using the Auxip client (to find Auxip files) \n",
    "# then the Cadip client (to find Cadip chunk files)\n",
    "for client in [auxip_client, cadip_client]:\n",
    "\n",
    "    # Call the service to search the reception stations for new files in the date interval.\n",
    "    files = client.search_stations(start_date, stop_date)\n",
    "    \n",
    "    file_count = len(files)\n",
    "    assert file_count, f\"We should have at least one {client.station_name} file\"\n",
    "    print (f\"Found {file_count} {client.station_name} files\\n\")\n",
    "\n",
    "    # Print the first file metadata. It is in the STAC format.\n",
    "    print(f\"First {client.station_name} file:\\n{json.dumps(files[0], indent=2)}\\n\")\n",
    "\n",
    "    # By default, the files are returned sorted by the most recent first (by creation date)\n",
    "    ids=\"\\n\".join([f\"{f['properties']['created']} - {f['id']}\" for f in files[:10]])\n",
    "    print(f\"Most recent {client.station_name} IDs and datetimes:\\n{ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd078a49-5768-4291-99cb-1885de815391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort by +/- any property, e.g. by creation date ascending = the oldest first\n",
    "for client in [auxip_client, cadip_client]:    \n",
    "    files = client.search_stations(start_date, stop_date, sortby=\"+created\")\n",
    "    ids=\"\\n\".join([f\"{f['properties']['created']} - {f['id']}\" for f in files[:10]])\n",
    "    print(f\"Oldest {client.station_name} IDs and datetimes:\\n{ids}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b28f2-1a06-47ca-967c-1dba7d99aff3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Stage Auxip and Cadip files\n",
    "\n",
    "When RS-Server stages a file, it means to:\n",
    "1. Copy (=download) it from the reception station into the temporary S3 bucket.\n",
    "1. Publish its metadata into the STAC catalog and move it from the temporary into the final S3 bucket.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9238c-13fe-4116-b87e-513903a18813",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_s3_files = []\n",
    "for client in [auxip_client, cadip_client]:\n",
    "\n",
    "    # When searching stations, we can also limit the number of returned results.\n",
    "    # For this example, let's keep only one file.\n",
    "    files = client.search_stations(start_date, stop_date, limit=1)\n",
    "    assert len(files) == 1\n",
    "\n",
    "    # We stage by filename = the file ID\n",
    "    first_filename = files[0][\"id\"]\n",
    "\n",
    "    # We must give a temporary S3 bucket path where to copy the file from the station.\n",
    "    # Use our API key username so avoid conflicts with other users.\n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{client.apikey_user_login}/{client.station_name}\"\n",
    "    temp_s3_files.append (f\"{s3_path}/{first_filename}\") # save it for later\n",
    "\n",
    "    # We can also download the file locally to the server, but this is useful only in local mode\n",
    "    local_path = None\n",
    "\n",
    "    # Call the staging service\n",
    "    client.staging(first_filename, s3_path=s3_path, tmp_download_path=local_path)\n",
    "\n",
    "    # Then we can check when the staging has finished by calling the check status service\n",
    "    while True:\n",
    "        status = client.staging_status(first_filename)\n",
    "        print (f\"Staging status for {first_filename!r}: {status.value}\")\n",
    "        if status in [EDownloadStatus.DONE, EDownloadStatus.FAILED]:\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        sleep(1)        \n",
    "    assert status == EDownloadStatus.DONE, \"Staging has failed\"\n",
    "\n",
    "    # WARNING: the file is copied into the temporary S3 bucket but is not yet published into the catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba922daf-0aad-4cf8-8755-cd87e40e8942",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Use the STAC catalog\n",
    "\n",
    "The SpatioTemporal Asset Catalog (STAC) family of specifications aim to standardize the way geospatial asset metadata is structured and queried. \n",
    "\n",
    "A 'spatiotemporal asset' is any file that represents information about the Earth captured in a certain space and time. \n",
    "\n",
    "For more information, see: https://github.com/radiantearth/stac-api-spec/tree/main\n",
    "\n",
    "In this section, we will see how to use most of the RS-Server STAC catalog functionalities.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f4a28-49ac-4772-836f-2266d9920324",
   "metadata": {},
   "source": [
    "#### Add a new collection to the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5e620-02ca-4e6a-a7eb-d8d8e9143797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac import Collection, Extent, SpatialExtent, TemporalExtent\n",
    "\n",
    "COLLECTION = \"my_collection\"\n",
    "\n",
    "# Clean the existing collection, if any\n",
    "stac_client.remove_collection(COLLECTION)\n",
    "\n",
    "# In this tutorial, after each operation, we will validate that \n",
    "# our catalog is valid to the STAC format, but this is optional.\n",
    "stac_client.validate_all()\n",
    "\n",
    "# Add new collection \n",
    "response = stac_client.add_collection(\n",
    "    Collection(\n",
    "        id=COLLECTION,\n",
    "        description=None, # rs-client will provide a default description for us\n",
    "        extent=Extent(\n",
    "            spatial=SpatialExtent(bboxes=[-180.0, -90.0, 180.0, 90.0]),\n",
    "            temporal=TemporalExtent([start_date, stop_date])\n",
    "        )\n",
    "    ))\n",
    "response.raise_for_status()\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958319a-a576-4e26-82e7-082ba17985c2",
   "metadata": {},
   "source": [
    "#### Read collections from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46d4be-0c59-47f4-b0b1-f6880fca7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all my personal catalog collections\n",
    "for collection in stac_client.get_collections():\n",
    "    print(f\"I have collection: {collection} at {collection.self_href}\")\n",
    "\n",
    "# Get a specific collection information\n",
    "my_collection = stac_client.get_collection(collection_id=COLLECTION)\n",
    "print(f\"\\nCollection information from {my_collection.self_href}\\n{json.dumps(collection.to_dict(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9474a-be39-4983-9e27-d43046859ade",
   "metadata": {},
   "source": [
    "#### Add new items to the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb97df-6b77-4ac9-b42f-645d02ae964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystac.asset import Asset\n",
    "from pystac.item import Item\n",
    "\n",
    "# Simulated values\n",
    "WIDTH=2500\n",
    "HEIGHT=2500\n",
    "\n",
    "# We will add one Auxip and one Cadip file that were staged from the previous steps\n",
    "for temp_s3_file in temp_s3_files:\n",
    "\n",
    "    # Let's use STAC item ID = filename\n",
    "    print(f\"Add catalog item from: {temp_s3_file!r}\")\n",
    "    item_id = os.path.basename(temp_s3_file)\n",
    "\n",
    "    # The file path from the temp s3 bucket is given in the assets\n",
    "    assets = {\"file\": Asset(href=temp_s3_file)}\n",
    "\n",
    "    # Other hardcoded parameters for this demo\n",
    "    geometry = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [[[-180, -90], [180, -90], [180, 90], [-180, 90], [-180, -90]]],\n",
    "    }\n",
    "    bbox = [-180.0, -90.0, 180.0, 90.0]\n",
    "    now = datetime.now()\n",
    "    properties = {\n",
    "        \"gsd\": 0.12345,\n",
    "        \"width\": WIDTH,\n",
    "        \"height\": HEIGHT,\n",
    "        \"datetime\": datetime.now(),\n",
    "        \"proj:epsg\": 3857,\n",
    "        \"orientation\": \"nadir\",\n",
    "        \"owner_id\": stac_client.owner_id,\n",
    "    }\n",
    "\n",
    "    # WARNING: after this, the staged file is moved from the temporary into the final bucket,\n",
    "    # so this cell can be run only once, or you'll need to stage the files again from the previous section.\n",
    "\n",
    "    # Add item to the STAC catalog collection, check status is OK\n",
    "    item = Item(\n",
    "        id=item_id,\n",
    "        geometry=geometry,\n",
    "        bbox=bbox,\n",
    "        datetime=now,\n",
    "        properties=properties,\n",
    "        assets=assets)\n",
    "    response = stac_client.add_item(COLLECTION, item)\n",
    "    response.raise_for_status()\n",
    "    stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79202fe3-b6b0-4029-b0d8-c232ee1afbc2",
   "metadata": {},
   "source": [
    "#### Read items from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf1c7b-67bf-4108-898c-d3a74733183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the items from the catalog to check that they were inserted\n",
    "for temp_s3_file in temp_s3_files:\n",
    "    item_id = os.path.basename(temp_s3_file)\n",
    "    inserted_item = my_collection.get_item(item_id)\n",
    "    assert inserted_item, \"Item was not inserted\"\n",
    "    print (f\"Saved item in the catalog:\\n{json.dumps (inserted_item.to_dict(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb517c9-96e4-422e-84c6-b3a2316c8fe7",
   "metadata": {},
   "source": [
    "#### Search items from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4423c-9218-41ec-a9eb-ef9ca10fbd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For searching, we need to prefix our collection name by <owner_id>_\n",
    "owner_collection = f\"{stac_client.owner_id}_{COLLECTION}\"\n",
    "\n",
    "# Search by the last inserted item id\n",
    "search = stac_client.search(ids=[item_id], collections=[owner_collection])\n",
    "results = list(search.items_as_dicts())\n",
    "assert results, f\"There should be at least one item with id={item_id}\"\n",
    "print(f\"Found {len(results)} results for id={item_id}\")\n",
    "\n",
    "# Search by the 'width' and 'height' property using a CQL2 filter, \n",
    "# see: https://pystac-client.readthedocs.io/en/stable/tutorials/cql2-filter.html\n",
    "filter_on_dimensions = {\n",
    "    \"op\": \"and\",\n",
    "    \"args\": [\n",
    "        {\"op\": \"=\", \"args\" : [{\"property\": \"collection\"}, owner_collection]},\n",
    "        {\"op\": \"=\", \"args\" : [{\"property\": \"width\"}, WIDTH]},\n",
    "        {\"op\": \"=\", \"args\" : [{\"property\": \"height\"}, HEIGHT]},\n",
    "    ]\n",
    "}\n",
    "search = stac_client.search(filter=filter_on_dimensions)\n",
    "results = list(search.items_as_dicts())\n",
    "assert results, f\"There should be at least one item for width={WIDTH} height={HEIGHT}\"\n",
    "print(f\"\\nFound {len(results)} results for width={WIDTH} height={HEIGHT}\")\n",
    "for result in results:\n",
    "    print(f\"({result['collection']}) {result['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e71868-a755-4b29-a0f3-e736891d8a03",
   "metadata": {},
   "source": [
    "#### Remove an item from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaebbe1-9671-4664-8db4-2684806f8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all items before removing\n",
    "items_before = list(stac_client.get_collection(COLLECTION).get_items())\n",
    "print (f\"{len(items_before)} items before removing\")\n",
    "\n",
    "# If there is at least one item\n",
    "if items_before:\n",
    "\n",
    "    # Remove the first item\n",
    "    item_id = items_before[0].id\n",
    "    stac_client.remove_item (COLLECTION, item_id)\n",
    "    \n",
    "    # We should have one less item in the collection\n",
    "    items_after = list(stac_client.get_collection(COLLECTION).get_items())\n",
    "    assert len(items_after) == (len(items_before) - 1), \\\n",
    "        f\"There should be {len(items_before) - 2} items in the collection, but we have {len(items_after)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f9c3d-13c1-4410-8f5c-2c4402c3acb2",
   "metadata": {},
   "source": [
    "#### Remove a collection from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b2a49-bd43-48f6-8c4f-67f1cf0d0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "\n",
    "# Remove the collection\n",
    "#stac_client.remove_collection (COLLECTION)\n",
    "\n",
    "# It should not exist anymore: trying to get the collection should raise an Exception\n",
    "try:\n",
    "    stac_client.get_collection(COLLECTION+\"_\")\n",
    "    assert False, f\"The collection {COLLECTION!r} should have been removed\"\n",
    "\n",
    "# So it is normal that we have this exception\n",
    "except pystac_client.exceptions.APIError:\n",
    "    print (f\"The collection {COLLECTION!r} has been removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc7a02-5a20-49ab-8081-7c1d0caec45e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Search Cadip sessions\n",
    "\n",
    "All Cadip chunk files are attached to a single Cadip session.\n",
    "\n",
    "We can search Cadip sessions by parameters, or find information about a specific session ID.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81869f26-c21e-4359-a959-29534798e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search cadip sessions by date interval and platforms\n",
    "platforms = [EPlatform.S1A, EPlatform.S2B]\n",
    "sessions = cadip_client.search_sessions(start_date=start_date, stop_date=stop_date, platforms=platforms)\n",
    "\n",
    "session_count = len(sessions)\n",
    "assert session_count, \"We should have at least one Cadip session\"\n",
    "print (f\"Found {session_count} Cadip sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45253518-999f-4f2e-8759-d9a7f39a71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first cadip session metadata. It is in the STAC format.\n",
    "print(f\"First Cadip session:\\n{json.dumps(sessions[0], indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c7920-3942-4156-88a3-f503ddf7230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the Cadip session IDs\n",
    "ids=[s[\"id\"] for s in sessions]\n",
    "print_ids=\"\\n\".join(ids)\n",
    "print(f\"Cadip sessions IDs:\\n{print_ids}\")\n",
    "\n",
    "# Save the first session ID for later\n",
    "first_cadip_session = ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711fe8e-0cb6-4e9a-b0bd-42f76c97c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also search Cadip sessions by specific session IDs,\n",
    "# e.g. get information for the cadip sessions #2 and #3\n",
    "search_ids=ids[1:3]\n",
    "search_sessions = cadip_client.search_sessions(session_ids=search_ids)\n",
    "print(f\"Cadip sessions information:\\n{json.dumps(search_sessions, indent=2)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e25f0-332f-4a6d-8fa8-f7fc34f96c50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Exercises: call services manually\n",
    "\n",
    "</div>\n",
    "\n",
    "Run again the previous cells but this time: \n",
    "1. Check if your API key allows you to access other Cadip stations, and STAC catalog collections from other owners.\n",
    "    1. Use a RsClient instance with one of these other stations and owner IDs.\n",
    "    1. Check that you can read and download Cadip files.\n",
    "    1. Check that this owner ID is used in the STAC catalog.\n",
    "1. Search Auxip and Cadip files using a different time interval.\n",
    "1. Stage at least one different Auxip and Cadip file into the STAC catalog.\n",
    "1. Print one different Cadip session information.\n",
    "\n",
    "**NOTE**: you can also use the website OpenAPI Swagger UI to call RS-Server (see cell below).\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb904b7-cedf-4174-b227-1de8ad4b9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    print(f\"\"\"OpenAPI Swagger UI for:\n",
    "  - Auxip: {RSPY_HOST_AUXIP}\n",
    "  - Cadip: {RSPY_HOST_CADIP}\n",
    "  - STAC catalog: {RSPY_HOST_CATALOG}\"\"\")\n",
    "else:\n",
    "    print(f\"OpenAPI Swagger UI: {generic_client.rs_server_href}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73bfc1-d12b-4c6a-bf97-766e72b6fc66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## Prefect workflows\n",
    "\n",
    "Prefect is a workflow orchestration tool. We will use it to:\n",
    "* Stage multiple Auxip and Cadip files at once.\n",
    "* Run a simulated DPR processing on the staged files, and save results in the catalog.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9ae81-a627-4887-b462-97979004cdab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: initialisation\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d953a-a0e0-4f87-94b3-55ba9ee7ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# The workflow will stage all files between a date interval.\n",
    "# In this example, we will stage all the Cadip chunk files from the first session.\n",
    "# We extract the mission and date from the session ID: <mission>_YYYYmmdd<other_info>\n",
    "session_id = first_cadip_session\n",
    "print(f\"First Cadip session ID: {session_id!r}\")\n",
    "mission, date_and_other = session_id.split(\"_\")\n",
    "date = datetime.strptime (date_and_other[:8], \"%Y%m%d\")\n",
    "start_date = date # start from midnight\n",
    "stop_date = date + timedelta(days=1) # midnight the day after\n",
    "\n",
    "print(f\"Mission: {mission!r}\")\n",
    "print(f\"Date interval: '{start_date} -> {stop_date}'\")\n",
    "\n",
    "# Note: we will miss files if the session overlaps two days. \n",
    "# We could also get the time interval from the session information \n",
    "# but the simulated data used in this notebook is not relevant.\n",
    "session_info = cadip_client.search_sessions(session_ids=[session_id])\n",
    "print(f\"Date interval from the session information (not used): \"\n",
    "      f\"'{session_info[0]['properties']['start_datetime']} -> {session_info[0]['properties']['end_datetime']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a9bd0-ac99-4829-9094-c323f052505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a prerequisite, we must create manually the Auxip and Cadip \n",
    "# collections in the catalog, if they don't already exist.\n",
    "from pystac_client.exceptions import APIError\n",
    "from rs_workflows import staging\n",
    "\n",
    "for client in [auxip_client, cadip_client]:\n",
    "    collection_name = staging.create_collection_name(mission, client.station_name)\n",
    "\n",
    "    # Save the collection name for later\n",
    "    if client == auxip_client:\n",
    "        auxip_collection = collection_name\n",
    "    else:\n",
    "        cadip_collection = collection_name\n",
    "\n",
    "    # Try to get collection information\n",
    "    try:\n",
    "        stac_client.get_collection(collection_id=collection_name)\n",
    "        print(f\"Collection already exists: {collection_name!r}\")\n",
    "\n",
    "    # If it fails, this means that the collection doesn't exist, so create it\n",
    "    except APIError:\n",
    "\n",
    "        print(f\"Create collection: {collection_name!r}\")\n",
    "        response = stac_client.add_collection(\n",
    "            Collection(\n",
    "                id=collection_name,\n",
    "                description=None, # rs-client will provide a default description for us\n",
    "                extent=Extent(\n",
    "                    spatial=SpatialExtent(bboxes=[-180.0, -90.0, 180.0, 90.0]),\n",
    "                    temporal=TemporalExtent([start_date, stop_date])\n",
    "                )\n",
    "            ))\n",
    "        response.raise_for_status()\n",
    "        stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8958f1f-737a-4d1f-96c0-02756d1926af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Cadip chunk files\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4422d974-9e50-44ec-a7f9-66fc6f94f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nView Prefect flow runs from: {RSPY_PREFECT_URL}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432dfc6-aca9-46f4-8a0d-6749b11786bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tasks to be run in parallel\n",
    "MAX_WORKERS = 15\n",
    "\n",
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    cadip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{cadip_client.owner_id}/{cadip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_date,\n",
    "    stop_datetime=stop_date,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec3f6b-d18b-4907-a1f5-5eef47e27598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c67f70-38ae-4e82-b0b4-f52ab58bc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the staged Cadip files in the STAC catalog\n",
    "\n",
    "# For searching, we need to prefix our collection name by <owner_id>_\n",
    "owner_collection = f\"{stac_client.owner_id}_{cadip_collection}\"\n",
    "\n",
    "# Use a cql2 filter to search by session ID\n",
    "filter_on_session = {\n",
    "    \"op\": \"and\",\n",
    "    \"args\": [\n",
    "      {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, owner_collection]},\n",
    "      {\"op\": \"=\", \"args\": [{\"property\": \"cadip:session_id\"}, session_id]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "search = stac_client.search(filter=filter_on_session)\n",
    "results = list(search.items_as_dicts())\n",
    "assert len(results) > 0, f\"At least one Cadip files should be staged for session ID: {session_id!r}\"\n",
    "print (f\"\\n{len(results)} Cadip files are staged for session ID: {session_id!r}.\")\n",
    "print (f\"First one:\\n{json.dumps (results[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019b40-326c-4e3f-a9bd-33af6eaa26ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: stage Auxip files\n",
    "\n",
    "We need to pass Auxip files to the DPR processing. They must be staged into the catalog.\n",
    "\n",
    "As, for now, the DPR processing is only a simulation, we can pass any Auxip files.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f05d5-3ffc-4b7e-8d0d-cf2bafcfc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the 3 most recent files between today and any old date\n",
    "files = auxip_client.search_stations(\n",
    "    datetime(year=1970, month=1, day=1), \n",
    "    datetime.today(), \n",
    "    sortby=\"-created\",\n",
    "    limit=None) # NOTE: for now \"limit\" is not working well with \"sortby\" so don't use it\n",
    "\n",
    "# Only keep the first 3 files\n",
    "files = files[:3]\n",
    "\n",
    "# Save the IDs = the filenames\n",
    "auxip_files = [f[\"id\"] for f in files]\n",
    "print_ids = \"\\n\".join(auxip_files)\n",
    "print(f\"Auxip files: \\n{print_ids}\")\n",
    "\n",
    "# Save the min and max dates for these 3 files\n",
    "dates = [datetime.strptime (f[\"properties\"][\"created\"], \"%Y-%m-%dT%H:%M:%S.%fZ\") for f in files]\n",
    "start_date = min(dates) - timedelta(seconds=1) # remove 1 second because the interval is exclusive\n",
    "stop_date = max(dates) + timedelta(seconds=1) # add 1 second\n",
    "print(f\"\\nDate interval: '{start_date} -> {stop_date}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa6a29-69a9-4d23-b290-3475e80e5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staging workflow configuration\n",
    "config = staging.PrefectFlowConfig(\n",
    "    auxip_client, \n",
    "    mission, \n",
    "    s3_path = f\"s3://{TEMP_S3_BUCKET}/{auxip_client.owner_id}/{auxip_client.station_name}\",\n",
    "    tmp_download_path=None, # no local download\n",
    "    max_workers=MAX_WORKERS,\n",
    "    start_datetime=start_date,\n",
    "    stop_datetime=stop_date,\n",
    "    limit=None) # no limit on the number of files\n",
    "\n",
    "# Start the prefect flow\n",
    "staging.staging_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a50f12-f08f-44c9-9449-bd413e836952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4841f-3798-4324-ade9-cd63ca76ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These 3 items should be in the STAC catalog, in the Auxip collection\n",
    "\n",
    "# Be sure that we don't have any duplicate filenames\n",
    "auxip_files = list(set(auxip_files))\n",
    "\n",
    "# Search by ID and collection\n",
    "owner_collection = f\"{stac_client.owner_id}_{auxip_collection}\"\n",
    "search = stac_client.search(ids=auxip_files, collections=[owner_collection])\n",
    "results = list(search.items_as_dicts())\n",
    "assert len(results) == len(auxip_files), f\"{len(results)} Auxip files were staged, we expected {len(auxip_files)}\"\n",
    "print(f\"Staged Auxip files:\\n\" + \"\\n\".join(auxip_files))\n",
    "print (f\"\\nFirst one:\\n{json.dumps (results[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19670b80-2a72-4e8d-b6c1-ff5b8c7f576e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "### Workflow: DPR simulator\n",
    "\n",
    "For now, this simulated DPR processor takes any input and writes any output.\n",
    "\n",
    "We use it to simulate a L0 processing that takes staged Cadip chunk files and Auxip files as input, and writes raw L0 products as output.\n",
    "\n",
    "[Back to top](#Contents)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8aaa8d-e0e8-468c-bbe2-d904eef0ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rs_workflows import s1_l0\n",
    "\n",
    "# The product types to process can be any of these 4 values\n",
    "product_types = [\"S1SEWRAW\", \"S1SIWRAW\", \"S1SSMRAW\", \"S1SWVRAW\"]\n",
    "\n",
    "# DPR workflow configuration\n",
    "config = s1_l0.PrefectS1L0FlowConfig(\n",
    "    stac_client,\n",
    "    RSPY_DPR_SIMU_URL,\n",
    "    mission,\n",
    "    session_id,\n",
    "    product_types,\n",
    "    auxip_files,\n",
    "    s3_path = f\"s3://{FINAL_S3_BUCKET}/{stac_client.owner_id}/DPR_S1L0\",\n",
    "    temp_s3_path = f\"s3://{TEMP_S3_BUCKET}/{stac_client.owner_id}/DPR_S1L0\",\n",
    ")\n",
    "\n",
    "# Start the prefect flow\n",
    "s1_l0.s1_l0_flow(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30079d-0c85-41b3-9547-d9ad5de7ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our catalog is valid to the STAC format.\n",
    "stac_client.validate_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912853c-87e9-464b-9611-91a79f78ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output products in the STAC catalog\n",
    "\n",
    "# The DPR collection name is hardcoded in the workflow source code\n",
    "dpr_collection = f\"{mission}_dpr\"\n",
    "dpr_products = list(stac_client.get_collection(dpr_collection).get_items())\n",
    "\n",
    "assert len(dpr_products) > 0, f\"At least one DPR product should be saved in the catalog.\"\n",
    "print_ids = \"\\n\".join([product.id for product in dpr_products])\n",
    "print (f\"\\n{len(dpr_products)} DPR products are saved in the catalog:\\n{print_ids}\")\n",
    "print (f\"\\nFirst one:\\n{json.dumps (dpr_products[0].to_dict(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582507d-5c1f-41bb-88e2-d794eccef131",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "\n",
    "### Exercises: workflows\n",
    "\n",
    "</div>\n",
    "\n",
    "Run again the previous cells but this time: \n",
    "* Use Cadip chunk files from a different Cadip session.\n",
    "* Use any other Auxip files.\n",
    "* Check that Cadip and Auxip files are staged in the STAC catalog.\n",
    "* Check that the simulated L0 products are staged in the STAC catalog.\n",
    "\n",
    "[Back to top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43fa8c-4558-41d6-92cf-70ceb7764e79",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"https://github.com/RS-PYTHON\" target=\"_blank\">View on GitHub</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
