{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca37ce2-b9cf-4f3e-8ad0-c83f6c0fc975",
   "metadata": {},
   "source": [
    "## Test the /search endpoints for auxip and cadip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1cb06-a7b6-40f9-b9f6-1ae0bf1025ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment before running a demo notebook.\n",
    "from resources.utils import *\n",
    "init_demo()\n",
    "from resources.utils import * # reload the global vars again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc00c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from urllib.parse import unquote\n",
    "\n",
    "!pip install iso8601 rfc3339\n",
    "import iso8601\n",
    "import rfc3339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b848f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Error:\n",
    "    \"\"\"One error from these tests.\"\"\"\n",
    "\n",
    "    endpoint: str\n",
    "    params: dict\n",
    "    messages: list[str]\n",
    "\n",
    "    def __repr__(self):\n",
    "        sep = \"\\n  - \"\n",
    "        return (\n",
    "            f\"\\nPOST {self.endpoint!r}\\n{json.dumps(self.params, indent=2)}\\n\"\n",
    "            f\"Error(s):{sep}{sep.join(self.messages)}\\n\"\n",
    "        )\n",
    "\n",
    "# List of errors\n",
    "errors: list[Error] = []\n",
    "\n",
    "def save_error(*args) -> None:\n",
    "    \"\"\"Print and save an error\"\"\"\n",
    "    error = Error(*args)\n",
    "    print(error, file=sys.stderr)\n",
    "    errors.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any collection name to search\n",
    "adgs_collection = \"adgs\"\n",
    "cadip_collection = \"cadip\"\n",
    "\n",
    "# Collections to search. If None: search all collections.\n",
    "collections = None\n",
    "\n",
    "# Parameters on which we can sort\n",
    "adgs_sortbys = [\n",
    "    \"id\",\n",
    "    \"auxip:id\",\n",
    "    \"file:size\",\n",
    "    \"type\",\n",
    "    \"eviction_datetime\",\n",
    "    \"created\",\n",
    "    \"start_datetime\",\n",
    "    \"end_datetime\"\n",
    "    ]\n",
    "cadip_sortbys = [\n",
    "    \"id\",\n",
    "    \"start_datetime\",\n",
    "    \"datetime\",\n",
    "    \"end_datetime\",\n",
    "    \"published\",\n",
    "    \"platform\",\n",
    "    \"cadip:id\",\n",
    "    \"cadip:num_channels\",\n",
    "    \"cadip:station_unit_id\",\n",
    "    \"sat:absolute_orbit\",\n",
    "    \"cadip:acquisition_id\",\n",
    "    \"cadip:antenna_id\",\n",
    "    \"cadip:front_end_id\",\n",
    "    \"cadip:retransfer\",\n",
    "    \"cadip:antenna_status_ok\",\n",
    "    \"cadip:front_end_status_ok\",\n",
    "    \"cadip:planned_data_start\",\n",
    "    \"cadip:planned_data_stop\",\n",
    "    \"cadip:downlink_status_ok\",\n",
    "    \"cadip:delivery_push_ok\"\n",
    "]\n",
    "sortbys = None\n",
    "\n",
    "# auxip or cadip /search endpoint\n",
    "auxip = False\n",
    "cadip = False\n",
    "endpoint = \"\"\n",
    "\n",
    "def ignore_search(search_properties=[]):\n",
    "    \"\"\"\n",
    "    Our adgs and cadip station simulators don't handle some parameters so we ignore these calls.\n",
    "    \"\"\"\n",
    "    # \"Too complex for adgs sim\"\n",
    "    if auxip:\n",
    "        if \"platform\" in search_properties:\n",
    "            return True\n",
    "        if search_properties == (\"constellation\", \"product:type\"):\n",
    "            return True\n",
    "\n",
    "    # Don't ignore, do the call\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_check(params={}, from_feature=None, search_properties=[]):\n",
    "    \"\"\"\n",
    "    Call the /search endpoint, check the results, return features and links.\n",
    "\n",
    "    The search parameters are in the `params` argument.\n",
    "\n",
    "    If `from_feature` and `search_properties` are set, these search property values \n",
    "    are copied from this reference feature.    \n",
    "    \"\"\"\n",
    "\n",
    "    info = f\"POST {endpoint!r} with params: {list(search_properties) + list(params.keys())}\"\n",
    "\n",
    "    # Ignore this call\n",
    "    if ignore_search(search_properties):\n",
    "        print(f\"Ignore {info}\")\n",
    "        return None, None\n",
    "    print(f\"Call {info}\")\n",
    "\n",
    "    # Set the query parameters\n",
    "    if collections:\n",
    "        params[\"collections\"] = collections\n",
    "    for property in search_properties:\n",
    "        in_value = from_feature.get(property) or from_feature[\"properties\"].get(property)\n",
    "\n",
    "        if property == \"id\":\n",
    "            params[\"ids\"] = [in_value]\n",
    "            \n",
    "        elif property == \"datetime\":\n",
    "            params[\"datetime\"] = in_value\n",
    "\n",
    "        # query parameters\n",
    "        else:\n",
    "            params.setdefault(\"query\", {})[property] = {\"eq\": in_value}\n",
    "\n",
    "    # Format datetime\n",
    "    try:\n",
    "        params[\"datetime\"] = params[\"datetime\"].replace(\"+00:00\", \"Z\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Call the search endpoint, read the returned features\n",
    "    try:\n",
    "        response = http_session.post(endpoint, json=params)\n",
    "    except Exception as exception:\n",
    "        save_error(endpoint, params, [str(exception)])\n",
    "        return None, None\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        save_error(endpoint, params, [f\"Status code: {response.status_code}\\n{unquote(response.content)}\"])\n",
    "        return None, None\n",
    "\n",
    "    ret_features = response.json()[\"features\"]\n",
    "    if not ret_features:\n",
    "        save_error(endpoint, params, [f\"No features returned\"])\n",
    "        return None, None\n",
    "\n",
    "    # Check that each returned feature property on which we filtered,\n",
    "    # has the same value than in the input feature.\n",
    "    messages: list[str] = []\n",
    "    for property in search_properties:\n",
    "        for ret_feature in ret_features:\n",
    "            in_value = from_feature.get(property) or from_feature[\"properties\"].get(property)\n",
    "            ret_value = ret_feature.get(property) or ret_feature[\"properties\"].get(property)\n",
    "\n",
    "            if in_value != ret_value:\n",
    "                messages.append(f\"Wrong {property!r}: {ret_value!r}, expected: {in_value!r}\")\n",
    "                break  # only print error for first wrong feature\n",
    "\n",
    "    if messages:\n",
    "        save_error(endpoint, params, messages)\n",
    "    return ret_features, response.json()[\"links\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01e358-0cd9-4605-afa5-f211a0df13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Main code #\n",
    "#############\n",
    "\n",
    "# Call auxip and cadip /search endpoint, with one collection # or all collections\n",
    "for (service, all_collections) in itertools.product((\"auxip\", \"cadip\"), (False,)): # True)):\n",
    "\n",
    "    # For better readability\n",
    "    auxip = service == \"auxip\"\n",
    "    cadip = service == \"cadip\"\n",
    "\n",
    "    # Init\n",
    "    if auxip:\n",
    "        endpoint = f\"{auxip_client.href_adgs}/auxip/search\"\n",
    "        one_collection = adgs_collection\n",
    "        sortby_date = \"created\"\n",
    "        sortbys = adgs_sortbys\n",
    "    elif cadip:\n",
    "        endpoint = f\"{cadip_client.href_cadip}/cadip/search\"\n",
    "        one_collection = cadip_collection\n",
    "        sortby_date = \"published\"\n",
    "        sortbys = cadip_sortbys\n",
    "\n",
    "    # Collections to search\n",
    "    collections = None if all_collections else [one_collection]\n",
    "\n",
    "    # Get all auxip products or cadip sessions, sorted by newest\n",
    "    all_features, _ = search_and_check({\n",
    "        \"limit\": 10000, \n",
    "        \"sortby\": [{\"direction\": \"desc\", \"field\": sortby_date}]})\n",
    "    \n",
    "    if all_features is None:\n",
    "        continue\n",
    "\n",
    "    ##################\n",
    "    # Test filtering #\n",
    "    ##################\n",
    "\n",
    "    # We take any existing feature returned by the stations, filter on its properties,\n",
    "    # and check that the filter was applied.\n",
    "    feature = all_features[1]\n",
    "\n",
    "    # All properties on which we can filter\n",
    "    properties = [\"id\", \"datetime\", \"platform\"]\n",
    "    if auxip:\n",
    "        properties += [\"constellation\", \"product:type\"]\n",
    "\n",
    "    # Test all combinations of n properties\n",
    "    for length in range(1, len(properties) + 1):\n",
    "        for search_properties in itertools.combinations(properties, length):\n",
    "            search_and_check({}, feature, search_properties)\n",
    "\n",
    "    ###################\n",
    "    # Test pagination #\n",
    "    ###################\n",
    "\n",
    "    # Get all auxip products or cadip sessions again, but with a temporal filter: \n",
    "    # remove the two most recent features.\n",
    "    # This is to test the pagination + filtering at the same time.\n",
    "    date_filter_stop = all_features[1][\"properties\"][\"datetime\"].replace(\"+00:00\", \"Z\")\n",
    "    date_filter_param = {\"datetime\": f\"1950-01-01T00:00:00Z/{date_filter_stop}\"}\n",
    "    date_filter_features, _ = search_and_check({\n",
    "        **date_filter_param,\n",
    "        \"limit\": 10000, \n",
    "        \"sortby\": [{\"direction\": \"desc\", \"field\": sortby_date}]})\n",
    "\n",
    "    # Test all sortby fields, with and without reverse, with and without a date filter\n",
    "    for (\n",
    "        sortby, \n",
    "        reverse, \n",
    "        reference_features\n",
    "    ) in itertools.product(\n",
    "        sortbys, \n",
    "        (False, True), \n",
    "        (all_features, date_filter_features)\n",
    "    ):\n",
    "        if not reference_features:\n",
    "            continue\n",
    "        \n",
    "        # Split the total number of features in n pages.\n",
    "        # 3 pages for the 1st sortby, a single one (this is faster) for the next\n",
    "        pages = 3 if sortby == sortbys[0] else 1\n",
    "        limit = math.ceil(len(reference_features) / pages)\n",
    "\n",
    "        # Endpoint parameters\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"sortby\": [{\"direction\": \"desc\" if reverse else \"asc\", \"field\": sortby}]\n",
    "        }\n",
    "        if reference_features == date_filter_features:\n",
    "            params.update(date_filter_param) # add filtering on date\n",
    "        page_params = params # will be update for next pages with a token\n",
    "\n",
    "        def sorted_value(feature: dict):\n",
    "            \"\"\"\n",
    "            If we sort by e.g. the datetime, return the input feature datetime. \n",
    "            It is either in the feature root, in its properties, or in the first asset.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                ret = (\n",
    "                    feature.get(sortby, None) or \n",
    "                    feature[\"properties\"].get(sortby, None) or\n",
    "                    list(feature[\"assets\"].values())[0].get(sortby, None))\n",
    "                if ret is None:\n",
    "                    raise KeyError\n",
    "                return ret\n",
    "            except (KeyError, IndexError):\n",
    "                raise RuntimeError(f\"Feature is missing field {sortby!r}:\\n{json.dumps(feature, indent=2)}\")\n",
    "\n",
    "        # Concatenate all values for the current sortby field \n",
    "        # that will be returned by the paginated and sorted endpoints\n",
    "        all_paginated_values = []\n",
    "\n",
    "        # The result should be the same as when we manually sort the values \n",
    "        # returned by the non-paginated endpoint.\n",
    "        try:\n",
    "            all_expected_values = sorted(\n",
    "                [sorted_value(feature) for feature in reference_features],\n",
    "                reverse=reverse)\n",
    "        except RuntimeError as error:\n",
    "            save_error(endpoint, page_params, [str(error)])\n",
    "            continue # try next sortby field\n",
    "\n",
    "        test_results = True\n",
    "\n",
    "        # For each page to request\n",
    "        for page in range(pages):\n",
    "\n",
    "            # Request current page\n",
    "            paginated_features, links = search_and_check(page_params)\n",
    "\n",
    "            # In case of error, don't process next pages\n",
    "            if not links:\n",
    "                test_results = False\n",
    "                break\n",
    "\n",
    "            # Read the previous (for page > 1) and next pages token = \n",
    "            # fetch the link which rel==\"previous\" or \"next\", and read its body/token value\n",
    "            try:\n",
    "                x_page_token = lambda x: \\\n",
    "                    [link for link in links if link[\"rel\"] == x][0][\"body\"][\"token\"]\n",
    "                next_page_token = x_page_token(\"next\")\n",
    "                prev_page_token = x_page_token(\"previous\") if page else None\n",
    "            except (KeyError, IndexError):\n",
    "                save_error(endpoint, page_params, [f\"Missing self/previous/next link(s)\"])\n",
    "                test_results = False\n",
    "                break # don't process next page\n",
    "\n",
    "            # Check the token for next and previous page\n",
    "            if f\"page={page+2}\" not in next_page_token:\n",
    "                save_error(endpoint, page_params, \n",
    "                    [f\"Wrong 'next' page token: {next_page_token!r}, should contain: 'page={page+2}'\"])\n",
    "                test_results = False\n",
    "                break # don't process next page\n",
    "            if prev_page_token and (f\"page={page}\" not in prev_page_token):\n",
    "                save_error(endpoint, page_params, \n",
    "                    [f\"Wrong 'previous' page token: {prev_page_token!r}, should contain: 'page={page}'\"])\n",
    "                \n",
    "            # Use the \"next\" token when requesting the next page\n",
    "            page_params.update({\"token\": next_page_token})\n",
    "\n",
    "            # Concatenate all values for the current sortby field \n",
    "            # returned by the paginated and sorted endpoint\n",
    "            try:\n",
    "                all_paginated_values += [sorted_value(feature) for feature in paginated_features]\n",
    "            except RuntimeError as error:\n",
    "                save_error(endpoint, page_params, [str(error)])\n",
    "                break # don't process next page\n",
    "\n",
    "        # After requesting all the pages, check that the results are sorted as expected\n",
    "        if test_results and (all_paginated_values != all_expected_values):\n",
    "            save_error(endpoint, page_params, [\n",
    "                f\"Paginated values (len={len(all_paginated_values)}) \"\n",
    "                f\"are not as expected (len={len(all_expected_values)}):\\n\" \n",
    "                f\"Got:      {all_paginated_values}\\n\"\n",
    "                f\"Expected: {all_expected_values}\"])\n",
    "\n",
    "    #############################\n",
    "    # Test the datetime formats #\n",
    "    #############################\n",
    "\n",
    "    # Get distinct datetimes from all features\n",
    "    all_datetimes = sorted([\n",
    "        iso8601.parse_date(feature[\"properties\"][\"datetime\"]) \n",
    "        for feature in all_features])\n",
    "    assert(len(all_datetimes) >= 5)\n",
    "\n",
    "    # Search by fixed datetime\n",
    "    fixed = all_datetimes[1]\n",
    "    fixed_str = rfc3339.rfc3339(fixed)\n",
    "\n",
    "    # Search by closed interval. \n",
    "    # The min and max datetimes should not be returned by the http request.\n",
    "    closed_str = f\"{rfc3339.rfc3339(all_datetimes[1])}/{rfc3339.rfc3339(all_datetimes[4])}\"\n",
    "    closed = all_datetimes[2:4]\n",
    "\n",
    "    # Search by from interval.  \n",
    "    # The min datetime should not be returned by the http request.\n",
    "    from_str = f\"{rfc3339.rfc3339(all_datetimes[-3])}/..\"\n",
    "    from_ = all_datetimes[-2:]\n",
    "\n",
    "    # Search by to interval.  \n",
    "    # The max datetime should not be returned by the http request.\n",
    "    to_str = f\"../{rfc3339.rfc3339(all_datetimes[2])}\"\n",
    "    to_ = all_datetimes[:2]\n",
    "\n",
    "    # Search features with the given datetime, return only the datetimes\n",
    "    def search_datetime(datetime_param: dict) -> tuple[list[datetime], list[str]]:\n",
    "        features, _ = search_and_check(datetime_param)\n",
    "        features = features or []\n",
    "        return sorted([\n",
    "            iso8601.parse_date(feature[\"properties\"][\"datetime\"]) \n",
    "            for feature in features])\n",
    "    \n",
    "    # Convert datetime list to str for printing\n",
    "    def dt_to_str(datetimes: list[datetime]) -> str:\n",
    "        return \"\\n    - \" + \"\\n    - \".join([rfc3339.rfc3339(d) for d in datetimes])\n",
    "\n",
    "    # Search with a fixed datetime\n",
    "    datetime_param = {\"datetime\": fixed_str}\n",
    "    datetimes = search_datetime(datetime_param)\n",
    "    if (len(datetimes) != 1) or (datetimes[0] != fixed):\n",
    "        save_error(endpoint, datetime_param, [\n",
    "            f\"Wrong datetime(s): {dt_to_str(datetimes)}\\n    Expected: {dt_to_str([fixed])}\"])\n",
    "        \n",
    "    # Search with a closed interval\n",
    "    datetime_param = {\"datetime\": closed_str}\n",
    "    datetimes = search_datetime(datetime_param)\n",
    "    if datetimes != closed:\n",
    "        save_error(endpoint, datetime_param, [\n",
    "            f\"Wrong datetime(s): {dt_to_str(datetimes)}\\n    Expected: {dt_to_str(closed)}\"])\n",
    "        \n",
    "    # Search with a from interval\n",
    "    datetime_param = {\"datetime\": from_str}\n",
    "    datetimes = search_datetime(datetime_param)\n",
    "    if datetimes != from_:\n",
    "        save_error(endpoint, datetime_param, [\n",
    "            f\"Wrong datetime(s): {dt_to_str(datetimes)}\\n    Expected: {dt_to_str(from_)}\"])\n",
    "        \n",
    "    # Search with a to interval\n",
    "    datetime_param = {\"datetime\": to_str}\n",
    "    datetimes = search_datetime(datetime_param)\n",
    "    if datetimes != to_:\n",
    "        save_error(endpoint, datetime_param, [\n",
    "            f\"Wrong datetime(s): {dt_to_str(datetimes)}\\n    Expected: {dt_to_str(to_)}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print again all errors at the end, exit the notebook with an error\n",
    "if errors:\n",
    "    message = \"\\n## Error message start ##\\n\"\n",
    "    for error in errors:\n",
    "        message += str(error)\n",
    "    message += \"\\n## Error message finish ##\\n\"\n",
    "    raise RuntimeError(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
